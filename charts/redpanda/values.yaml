# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# This file contains values for variables referenced from yaml files in the templates directory.
#
# For further information on Helm templating see the documentation at:
#  https://helm.sh/docs/chart_template_guide/values_files/

#
# >>> This chart requires Helm version 3.6.0 or greater <<<
#

# Common settings
#
# -- Override `redpanda.name` template.
nameOverride: ""
# -- Override `redpanda.fullname` template.
fullnameOverride: ""
# -- Default Kubernetes cluster domain.
clusterDomain: cluster.local
# -- Additional labels to add to all Kubernetes objects.
# For example, `my.k8s.service: redpanda`.
commonLabels: {}
# -- Node selection constraints for scheduling Pods, can override this for StatefulSets.
# For details,
# see the [Kubernetes documentation](https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector).
nodeSelector: {}
# -- Taints to be tolerated by Pods, can override this for StatefulSets.
# For details,
# see the [Kubernetes documentation](https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/).
tolerations: []

# -- Redpanda Docker image settings.
image:
  # -- Docker repository from which to pull the Redpanda Docker image.
  repository: vectorized/redpanda
  # -- The Redpanda version.
  # See DockerHub for:
  # [All stable versions](https://hub.docker.com/r/redpandadata/redpanda/tags)
  # and [all unstable versions](https://hub.docker.com/r/redpandadata/redpanda-unstable/tags).
  # @default -- `Chart.appVersion`.
  tag: ""
  # -- The imagePullPolicy.
  # If `image.tag` is 'latest', the default is `Always`.
  pullPolicy: IfNotPresent

# -- Enterprise license key (optional).
# For details,
# see the [License documentation](https://docs.redpanda.com/docs/get-started/licenses/?platform=kubernetes#redpanda-enterprise-edition).
license_key: ""
# -- Secret name and secret key where the license key is stored.
license_secret_ref: {}
  # secret_name: my-secret
  # secret_key: key-where-license-is-stored

# -- Rack Awareness settings.
# For details,
# see the [Rack Awareness documentation](https://docs.redpanda.com/docs/manage/kubernetes/kubernetes-rack-awareness/).
rackAwareness:
  # -- When running in multiple racks or availability zones, use a Kubernetes Node
  # annotation value as the Redpanda rack value.
  # Enabling this requires running with a service account with "get" Node permissions.
  # To have the Helm chart configure these permissions,
  # set `serviceAccount.create=true` and `rbac.enabled=true`.
  enabled: false
  # -- The common well-known annotation to use as the rack ID.
  # Override this only if you use a custom Node annotation.
  nodeAnnotation: topology.kubernetes.io/zone

#
# -- Redpanda Console settings.
# For a reference of configuration settings,
# see the [Redpanda Console documentation](https://docs.redpanda.com/docs/reference/console/config/).
console:
  enabled: true
  configmap:
    create: false
  secret:
    create: false
  deployment:
    create: false
  config: {}

# -- Authentication settings.
# For details,
# see the [SASL documentation](https://docs.redpanda.com/docs/manage/kubernetes/security/sasl-kubernetes/).
auth:
  sasl:
    # -- Enable SASL authentication.
    # If you enable SASL authentication, you must provide a Secret in `auth.sasl.secretRef`.
    enabled: false
    # -- The authentication mechanism to use for the superuser. Options are `SCRAM-SHA-256` and `SCRAM-SHA-512`.
    mechanism: SCRAM-SHA-512
    # -- A Secret that contains your superuser credentials.
    # The file must include an empty line at the end.
    # For details,
    # see the [SASL documentation](https://docs.redpanda.com/docs/manage/kubernetes/security/sasl-kubernetes/#use-secrets).
    secretRef: "redpanda-users"
    # -- Optional list of superusers.
    # These superusers will be created in the Secret whose name is defined in `auth.sasl.secretRef`.
    # If this list is empty,
    # the Secret in `auth.sasl.secretRef` must already exist in the cluster before you deploy the chart.
    users:
    - name: admin
      password: change-me
      mechanism: SCRAM-SHA-512

# -- TLS settings.
# For details, see the [TLS documentation](https://docs.redpanda.com/docs/manage/kubernetes/security/kubernetes-tls/).
tls:
  # -- Enable TLS globally for all listeners.
  # Each listener must include a Certificate name in its `<listener>.tls` object.
  # To allow you to enable TLS for individual listeners,
  # Certificates in `auth.tls.certs` are always loaded, even if `tls.enabled` is `false`.
  # See `listeners.<listener-name>.tls.enabled`.
  enabled: true
  # -- List all Certificates here,
  # then you can reference a specific Certificate's name
  # in each listener's `listeners.<listener name>.tls.cert` setting.
  certs:
    # -- This key is the Certificate name.
    # To apply the Certificate to a specific listener,
    # reference the Certificate's name in `listeners.<listener-name>.tls.cert`.
    default:
      # -- To use a custom pre-installed Issuer,
      # add its name and kind to the `issuerRef` object.
      # issuerRef:
      #   name: redpanda-default-root-issuer
      #   kind: Issuer   # Can be Issuer or ClusterIssuer
      # -- To use a secret with custom tls files,
      # secretRef:
      #  name: my-tls-secret
      # -- Set the `caEnabled` flag to `true` only for Certificates
      # that are not authenticated using public authorities.
      caEnabled: true
      # duration: 43800h
    # -- Example external tls configuration
    # uncomment and set the right key to the listeners that require them
    # also enable the tls setting for those listeners.
    # external:
      # -- To use a custom pre-installed Issuer,
      # add its name and kind to the `issuerRef` object.
      # issuerRef:
      #   name: redpanda-default-root-issuer
      #   kind: Issuer   # Can be Issuer or ClusterIssuer
      # -- To use a secret with custom tls files,
      # secretRef:
      #   name: my-tls-secret
      # -- Set the `caEnabled` flag to `true` only for Certificates
      # that are not authenticated using public authorities.
      # caEnabled: true
      # duration: 43800h

# -- External access settings.
# For details,
# see the [Networking and Connectivity documentation](https://docs.redpanda.com/docs/manage/kubernetes/networking/networking-and-connectivity/).
external:
  # -- Enable external access for each Service.
  # You can toggle external access for each listener in
  # `listeners.<service name>.external.<listener-name>.enabled`.
  enabled: true
  # -- External access type. Only `NodePort` and `LoadBalancer` are supported.
  # If undefined, then advertised listeners will be configured in Redpanda,
  # but the helm chart will not create a Service.
  # You must create a Service manually.
  # Warning: If you use LoadBalancers, you will likely experience higher latency and increased packet loss.
  # NodePort is recommended in cases where latency is a priority.
  type: NodePort
  # Optional source range for external access. Only applicable when external.type is LoadBalancer
  # sourceRanges: []
  # -- Optional domain advertised to external clients
  # If specified, then it will be appended to the `external.addresses` values as each broker's advertised address
  # domain: local
  # Optional list of addresses that the Redpanda brokers advertise.
  # Provide one entry for each broker in order of StatefulSet replicas.
  # The number of brokers is defined in statefulset.replicas.
  # The values can be IP addresses or DNS names.
  # If external.domain is set, the domain is appended to these values.
  # addresses:
  # - redpanda-0
  # - redpanda-1
  # - redpanda-2
  #
  # annotations:
    # For example:
    # cloud.google.com/load-balancer-type: "Internal"
    # service.beta.kubernetes.io/aws-load-balancer-type: nlb

# -- Log-level settings.
logging:
  # -- Log level
  # Valid values (from least to most verbose) are: `warn`, `info`, `debug`, and `trace`.
  logLevel: info
  # -- Send usage statistics back to Redpanda Data.
  # For details,
  # see the [stats reporting documentation](https://docs.redpanda.com/docs/cluster-administration/monitoring/#stats-reporting).
  usageStats:
    # Enable the `rpk.enable_usage_stats` property.
    enabled: true
    # Your organization name (optional)
    # organization: your-org
    # Your cluster ID (optional)
    # clusterId: your-helm-cluster

# -- Pod resource management.
# This section simplifies resource allocation
# by providing a single location where resources are defined.
# Helm sets these resource values within the `statefulset.yaml` and `configmap.yaml` templates.
#
# The default values are for a development environment.
# Production-level values and other considerations are documented,
# where those values are different from the default.
# For details,
# see the [Pod resources documentation](https://docs.redpanda.com/docs/manage/kubernetes/manage-resources/).
resources:
  #
  # -- CPU resources.
  # For details,
  # see the [Pod resources documentation](https://docs.redpanda.com/docs/manage/kubernetes/manage-resources/#configure-cpu-resources).
  cpu:
    # -- Redpanda makes use of a thread per core model.
    # For details, see this [blog](https://redpanda.com/blog/tpc-buffers).
    # For this reason, Redpanda should only be given full cores.
    #
    # Note: You can increase cores, but decreasing cores is not currently supported.
    # See the [GitHub issue](https://github.com/redpanda-data/redpanda/issues/350).
    #
    # This setting is equivalent to `--smp`, `resources.requests.cpu`, and `resources.limits.cpu`.
    # For production, use `4` or greater.
    cores: 1
    #
    # -- Overprovisioned means Redpanda won't assume it has all of the provisioned CPU.
    # This should be true unless the container has CPU affinity.
    # Equivalent to: `--idle-poll-time-us 0 --thread-affinity 0 --poll-aio 0`
    #
    # If the value of full cores in `resources.cpu.cores` is less than `1`, this
    # setting is set to `true`.
    # overprovisioned: false
  #
  # -- Memory resources
  # For details,
  # see the [Pod resources documentation](https://docs.redpanda.com/docs/manage/kubernetes/manage-resources/#configure-memory-resources).
  memory:
    # Enables memory locking.
    # For production, set to `true`.
    # enable_memory_locking: false
    #
    # It is recommended to have at least 2Gi of memory per core for the Redpanda binary.
    # This memory is taken from the total memory given to each container.
    # The Helm chart allocates 80% of the container's memory to Redpanda, leaving the rest for
    # the Seastar subsystem (reserveMemory) and other container processes.
    # So at least 2.5Gi per core is recommended in order to ensure Redpanda has a full 2Gi.
    #
    # These values affect `--memory` and `--reserve-memory` flags passed to Redpanda and the memory
    # requests/limits in the StatefulSet.
    # Valid suffixes: B, K, M, G, Ki, Mi, and Gi
    #
    container:
      # Minimum memory count for each Redpanda broker.
      # If omitted, the `min` value is equal to the `max` value (requested resources defaults to limits).
      # This setting is equivalent to `resources.requests.memory`.
      # For production, use 10Gi or greater.
      # min: 2.5Gi
      #
      # -- Maximum memory count for each Redpanda broker.
      # Equivalent to `resources.limits.memory`.
      # For production, use `10Gi` or greater.
      max: 2.5Gi
    #
    # This optional `redpanda` object allows you to specify the memory size for both the Redpanda
    # process and the underlying reserved memory used by Seastar.
    # This section is omitted by default, and memory sizes are calculated automatically
    # based on container memory.
    # Uncommenting this section and setting memory and reserveMemory values will disable
    # automatic calculation.
    #
    # If you are setting the following values manually, keep in mind the following guidelines.
    # Getting this wrong may lead to performance issues, instability, and loss of data:
    # The amount of memory to allocate to a container is determined by the sum of three values:
    # 1. Redpanda (at least 2Gi per core, ~80% of the container's total memory)
    # 2. Seastar subsystem (200Mi * 0.2% of the container's total memory, 200Mi < x < 1Gi)
    # 3. Other container processes (whatever small amount remains)
    # redpanda:
      # Memory for the Redpanda process.
      # This must be lower than the container's memory (resources.memory.container.min if provided, otherwise
      # resources.memory.container.max).
      # Equivalent to --memory.
      # For production, use 8Gi or greater.
      # memory: 2Gi
      #
      # Memory reserved for the Seastar subsystem.
      # Any value above 1Gi will provide diminishing performance benefits.
      # Equivalent to --reserve-memory.
      # For production, use 1Gi.
      # reserveMemory: 200Mi

# -- Persistence settings.
# For details, see the [storage documentation](https://docs.redpanda.com/docs/manage/kubernetes/configure-storage/).
storage:
  # -- Absolute path on the host to store Redpanda's data.
  # If unspecified, then an `emptyDir` volume is used.
  # If specified but `persistentVolume.enabled` is true, `storage.hostPath` has no effect.
  hostPath: ""
  # -- If `persistentVolume.enabled` is true, a PersistentVolumeClaim is created and
  # used to store Redpanda's data. Otherwise, `storage.hostPath` is used.
  persistentVolume:
    enabled: true
    size: 20Gi
    # -- To disable dynamic provisioning, set to "-".
    # If undefined or empty (default), then no storageClassName spec is set,
    # and the default dynamic provisioner is chosen (gp2 on AWS, standard on
    # GKE, AWS & OpenStack).
    storageClass: ""
    # -- Additional labels to apply to the created PersistentVolumeClaims.
    labels: {}
    # -- Additional annotations to apply to the created PersistentVolumeClaims.
    annotations: {}
  #
  # Settings for the Tiered Storage cache.
  # For details,
  # see the [Tiered Storage documentation](https://docs.redpanda.com/docs/manage/kubernetes/tiered-storage/#caching).
  # For the maximum size of the disk cache, see `tieredConfig.cloud_storage_cache_size`.
  #
  # -- Absolute path on the host to store Redpanda's Tiered Storage cache.
  # If unspecified, then an `emptyDir` volume is used.
  # If specified but `tieredStoragePersistentVolume.enabled` is `true`, `storage.tieredStorageHostPath` has no effect.
  tieredStorageHostPath: ""
  # If `tieredStoragePersistentVolume.enabled` is true,
  # a PersistentVolumeClaim is created for the Tiered Storage cache and
  # used to store data retrieved from cloud storage, such as S3). Otherwise `storage.tieredStorageHostPath` is used.
  tieredStoragePersistentVolume:
    enabled: false
    # -- To disable dynamic provisioning, set to "-".
    # If undefined or empty (default), then no storageClassName spec is set,
    # and the default dynamic provisioner is chosen (gp2 on AWS, standard on
    # GKE, AWS & OpenStack).
    storageClass: ""
    # -- Additional labels to apply to the created PersistentVolumeClaims.
    labels: {}
    # -- Additional annotations to apply to the created PersistentVolumeClaims.
    annotations: {}
  #
  # -- Tiered Storage settings
  # Requires `license_key` or `license_secret_ref`
  # For details,
  # see the [Tiered Storage documentation](https://docs.redpanda.com/docs/manage/kubernetes/tiered-storage/).
  tieredConfig:
    # -- Global flag that enables Tiered Storage if a license key is provided.
    # See the [property reference documentation](https://docs.redpanda.com/docs/reference/cluster-properties/#cloud_storage_enabled).
    cloud_storage_enabled: false
    # -- See the [property reference documentation](https://docs.redpanda.com/docs/reference/tunable-properties/#cloud_storage_enable_remote_write).
    cloud_storage_enable_remote_write: true
    # -- See the [property reference documentation](https://docs.redpanda.com/docs/reference/tunable-properties/#cloud_storage_enable_remote_read).
    cloud_storage_enable_remote_read: true

    # -- Required for AWS and GCS.
    # See the [property reference documentation](https://docs.redpanda.com/docs/reference/cluster-properties/#cloud_storage_region).
    cloud_storage_region: ""
    # -- Required for AWS and GCS.
    # See the [property reference documentation](https://docs.redpanda.com/docs/reference/cluster-properties/#cloud_storage_bucket).
    cloud_storage_bucket: ""
    # -- Required for AWS and GCS authentication with access keys.
    # See the [property reference documentation](https://docs.redpanda.com/docs/reference/cluster-properties/#cloud_storage_access_key).
    cloud_storage_access_key: ""
    # -- Required for AWS and GCS authentication with access keys.
    # See the [property reference documentation](https://docs.redpanda.com/docs/reference/cluster-properties/#cloud_storage_secret_key).
    cloud_storage_secret_key: ""
    # -- See the [property reference documentation](https://docs.redpanda.com/docs/reference/cluster-properties/#cloud_storage_api_endpoint).
    cloud_storage_api_endpoint: ""
    # -- Required for ABS.
    # See the [property reference documentation](https://docs.redpanda.com/docs/reference/cluster-properties/#cloud_storage_azure_container).
    cloud_storage_azure_container: null
    # -- Required for ABS.
    # See the [property reference documentation](https://docs.redpanda.com/docs/reference/cluster-properties/#cloud_storage_azure_storage_account).
    cloud_storage_azure_storage_account: null
    # -- Required for ABS.
    # See the [property reference documentation](https://docs.redpanda.com/docs/reference/cluster-properties/#cloud_storage_azure_shared_key).
    cloud_storage_azure_shared_key: null
    # Available starting from 22.3.X
    # -- Required for AWS and GCS authentication with IAM roles.
    # See the [property reference documentation](https://docs.redpanda.com/docs/reference/cluster-properties/#cloud_storage_credentials_source).
    cloud_storage_credentials_source: config_file

    # -- Maximum size of the disk cache used by Tiered Storage.
    # Default is 20 GiB.
    # See the [property reference documentation](https://docs.redpanda.com/docs/reference/cluster-properties/#cloud_storage_cache_size).
    cloud_storage_cache_size: 21474836480
    # cloud_storage_cache_directory: ""
    # cloud_storage_cache_check_interval: 30000
    # cloud_storage_initial_backoff_ms: 100
    # cloud_storage_max_connections: 20
    # cloud_storage_segment_upload_timeout_ms: 30000
    # cloud_storage_manifest_upload_timeout_ms: 10000
    # cloud_storage_max_connection_idle_time_ms: 5000
    # cloud_storage_idle_timeout_ms: 10000
    # cloud_storage_segment_max_upload_interval_sec: 1
    # cloud_storage_trust_file: ""
    # cloud_storage_upload_ctrl_update_interval_ms: 60000
    # cloud_storage_upload_ctrl_p_coeff: -2
    # cloud_storage_upload_ctrl_d_coeff: 0
    # cloud_storage_upload_ctrl_min_shares: 100
    # cloud_storage_upload_ctrl_max_shares: 1000
    # DEPRECATED: cloud_storage_reconciliation_interval_ms: 10000
    # cloud_storage_disable_tls: false
    # cloud_storage_api_endpoint_port: 443
    # cloud_storage_idle_threshold_rps: 1
    # cloud_storage_enable_segment_merging: true
    # cloud_storage_segment_size_target: # The default segment size is controlled by log_segment_size
    # cloud_storage_segment_size_min: # Default is 50% of log segment size

post_install_job:
  enabled: true
  # Resource requests and limits for the post-install batch job
  # resources:
  #   requests:
  #     cpu: 1
  #     memory: 512Mi
  #   limits:
  #     cpu: 2
  #     memory: 1024Mi
  # labels: {}
  # annotations: {}

post_upgrade_job:
  enabled: true
  # Resource requests and limits for the post-upgrade batch job
  # resources:
  #   requests:
  #     cpu: 1
  #     memory: 512Mi
  #   limits:
  #     cpu: 2
  #     memory: 1024Mi
  # labels: {}
  # annotations: {}
  # Additional environment variables for the Post Upgrade Job
  # extraEnv:
  #   - name: AWS_SECRET_ACCESS_KEY
  #     valueFrom:
  #       secretKeyRef:
  #         name: my-secret
  #         key: redpanda-aws-secret-access-key
  # Additional environment variables for the Post Upgrade Job mapped from Secret or ConfigMap
  # extraEnvFrom:
  #   - secretRef:
  #       name: redpanda-aws-secrets

statefulset:
  # -- Number of Redpanda brokers (Redpanda Data recommends setting this to the number of worker nodes in the cluster)
  replicas: 3
  updateStrategy:
    type: RollingUpdate
  budget:
    maxUnavailable: 1
  # -- Additional annotations to apply to the Pods of this StatefulSet.
  annotations: {}
  # -- Adjust the period for your probes to meet your needs.
  # For details,
  # see the [Kubernetes documentation](https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes).
  startupProbe:
    initialDelaySeconds: 1
    failureThreshold: 120
    periodSeconds: 10
  livenessProbe:
    initialDelaySeconds: 10
    failureThreshold: 3
    periodSeconds: 10
  readinessProbe:
    initialDelaySeconds: 1
    failureThreshold: 3
    periodSeconds: 10
    successThreshold: 1
  #
  # StatefulSet resources:
  # Resources are set through the top-level resources section above.
  # It is recommended to set resources values in that section rather than here, as this will guarantee
  # memory is allocated across containers, Redpanda, and the Seastar subsystem correctly.
  # This automatic memory allocation is in place because Repanda and the Seastar subsystem require flags
  # at startup that set the amount of memory available to each process.
  # Kubernetes (mainly statefulset), Redpanda, and Seastar memory values are tightly coupled.
  # Adding a resource section here will be ignored.
  #
  # -- Inter-Pod Affinity rules for scheduling Pods of this StatefulSet.
  # For details,
  # see the [Kubernetes documentation](https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#inter-pod-affinity-and-anti-affinity).
  podAffinity: {}
  # -- Anti-affinity rules for scheduling Pods of this StatefulSet.
  # For details,
  # see the [Kubernetes documentation](https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#inter-pod-affinity-and-anti-affinity).
  # You may either edit the default settings for anti-affinity rules,
  # or specify new anti-affinity rules to use instead of the defaults.
  podAntiAffinity:
    # -- The topologyKey to be used.
    # Can be used to spread across different nodes, AZs, regions etc.
    topologyKey: kubernetes.io/hostname
    # -- Valid anti-affinity types are `soft`, `hard`, or `custom`.
    # Use `custom` if you want to supply your own anti-affinity rules in the `podAntiAffinity.custom` object.
    type: hard
    # -- Weight for `soft` anti-affinity rules.
    # Does not apply for other anti-affinity types.
    weight: 100
    # -- Change `podAntiAffinity.type` to `custom` and provide your own podAntiAffinity rules here.
    custom: {}
  # -- Node selection constraints for scheduling Pods of this StatefulSet.
  # These constraints override the global nodeSelector value.
  # For details,
  # see the [Kubernetes documentation](https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector).
  nodeSelector: {}
  # -- PriorityClassName given to Pods of this StatefulSet.
  # For details,
  # see the [Kubernetes documentation](https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/#priorityclass).
  priorityClassName: ""
  # -- Taints to be tolerated by Pods of this StatefulSet.
  # These tolerations override the global tolerations value.
  # For details,
  # see the [Kubernetes documentation](https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/).
  tolerations: []
  # For details,
  # see the [Kubernetes documentation](https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/).
  topologySpreadConstraints:
    maxSkew: 1
    topologyKey: topology.kubernetes.io/zone
    whenUnsatisfiable: ScheduleAnyway
  securityContext:
    fsGroup: 101
    runAsUser: 101
    fsGroupChangePolicy: OnRootMismatch
  initContainers:
    tuning:
      resources: {}
    setDataDirOwnership:
      # -- In environments where root is not allowed, you cannot change the ownership of files and directories.
      # Enable `setDataDirOwnership` when using default minikube cluster configuration.
      enabled: false
      resources: {}
    setTieredStorageCacheDirOwnership:
      resources: {}
    configurator:
      resources: {}
  initContainerImage:
    repository: busybox
    tag: latest
  # -- Additional flags to pass to redpanda,
  additionalRedpandaCmdFlags: []
#    - --unsafe-bypass-fsync

# -- Service account management.
serviceAccount:
  # -- Specifies whether a service account should be created.
  create: false
  # -- Annotations to add to the service account.
  annotations: {}
  # -- The name of the service account to use.
  # If not set and `serviceAccount.create` is `true`,
  # a name is generated using the `redpanda.fullname` template.
  name: ""

# -- Role Based Access Control.
rbac:
  # -- Enable for features that need extra privileges.
  enabled: false
  # -- Annotations to add to the `rbac` resources.
  annotations: {}

# -- Redpanda tuning settings.
# Each is set to their default values in Redpanda.
tuning:
  # -- Increase the maximum number of outstanding asynchronous IO operations if the
  # current value is below a certain threshold. This allows Redpanda to make as many
  # simultaneous IO requests as possible, increasing throughput.
  #
  # When this option is enabled, Helm creates a privileged container. If your security profile does not allow this,
  # see the [tuning documentation](https://docs.redpanda.com/docs/deploy/deployment-option/self-hosted/kubernetes/kubernetes-tune-workers/).
  tune_aio_events: true
  #
  # Syncs NTP
  # tune_clocksource: false
  #
  # Creates a "ballast" file so that, if a Redpanda node runs out of space,
  # you can delete the ballast file to allow the node to resume operations and then
  # delete a topic or records to reduce the space used by Redpanda.
  # tune_ballast_file: false
  #
  # The path where the ballast file will be created.
  # ballast_file_path: "/var/lib/redpanda/data/ballast"
  #
  # The ballast file size.
  # ballast_file_size: "1GiB"
  #
  # (Optional) The vendor, VM type and storage device type that redpanda will run on, in
  # the format <vendor>:<vm>:<storage>. This hints to rpk which configuration values it
  # should use for the redpanda IO scheduler.
  # Some valid values are "gcp:c2-standard-16:nvme", "aws:i3.xlarge:default"
  # well_known_io: ""
  #
  # The following tuning parameters must be false in container environments and will be ignored:
  #   tune_network
  #   tune_disk_scheduler
  #   tune_disk_nomerges
  #   tune_disk_irq
  #   tune_fstrim
  #   tune_cpu
  #   tune_swappiness
  #   tune_transparent_hugepages
  #   tune_coredump


# -- Listener settings.
#
# Override global settings configured above for individual
# listeners.
# For details,
# see the [listeners documentation](https://docs.redpanda.com/docs/beta/manage/kubernetes/networking/configure-listeners/).
listeners:
  # -- Admin API listener (only one).
  admin:
    # -- The port for both internal and external connections to the Admin API.
    port: 9644
    # -- Optional external access settings.
    external:
      # -- Name of the external listener.
      default:
        # Override the global `external.enabled` for only this listener.
        # enabled: true
        # -- The port advertised to this listener's external clients.
        # List one port if you want to use the same port for each broker (would be the case when using NodePort service).
        # Otherwise, list the port you want to use for each broker in order of StatefulSet replicas.
        # If undefined, `listeners.admin.port` is used.
        advertisedPorts:
        - 31644
    # -- Optional TLS section (required if global TLS is enabled)
    tls:
      # Optional flag to override the global TLS enabled flag.
      # enabled: true
      # -- Name of the Certificate used for TLS (must match a Certificate name that is registered in tls.certs).
      cert: default
      # -- If true, the truststore file for this listener is included in the ConfigMap.
      requireClientAuth: false
  # -- Kafka API listeners.
  kafka:
    # -- The port for internal client connections.
    port: 9093
    tls:
      # Optional flag to override the global TLS enabled flag.
      # enabled: true
      cert: default
      requireClientAuth: false
    external:
      default:
        # enabled: true
        # -- The port used for external client connections.
        port: 9094
        # -- If undefined, `listeners.kafka.external.default.port` is used.
        advertisedPorts:
        - 31092
        # -- Uncomment to define external tls
        # tls:
        #   # Optional flag to override the global TLS enabled flag.
        #   # enabled: true
        #   cert: external
  # -- RPC listener (this is never externally accessible).
  rpc:
    port: 33145
    tls:
      # Optional flag to override the global TLS enabled flag.
      # enabled: true
      cert: default
      requireClientAuth: false
  # -- Schema registry listeners.
  schemaRegistry:
    enabled: true
    port: 8081
    kafkaEndpoint: default
    tls:
      # Optional flag to override the global TLS enabled flag.
      # enabled: true
      cert: default
      requireClientAuth: false
    external:
      default:
        # enabled: true
        port: 8084
        advertisedPorts:
        - 30081
        # -- Uncomment to define external tls
        # tls:
        #   # Optional flag to override the global TLS enabled flag.
        #   # enabled: true
        #   cert: external
  # -- HTTP API listeners (aka PandaProxy).
  http:
    enabled: true
    port: 8082
    kafkaEndpoint: default
    tls:
      # Optional flag to override the global TLS enabled flag.
      # enabled: true
      cert: default
      requireClientAuth: false
    external:
      default:
        # enabled: true
        port: 8083
        advertisedPorts:
          - 30082
        # -- Uncomment to define external tls
        # tls:
        # #  Optional flag to override the global TLS enabled flag.
        # #  enabled: true
        #   cert: external

# Expert Config
# Here be dragons!
#
# -- This section contains various settings supported by Redpanda that may not work
# correctly in a Kubernetes cluster. Changing these settings comes with some risk.
#
# Use these settings to customize various Redpanda configurations that are not covered in other sections.
# These values have no impact on the configuration or behavior of the Kubernetes objects deployed by Helm,
# and therefore should not be modified for the purpose of configuring those objects.
# Instead, these settings get passed directly to the Redpanda binary at startup.
# For descriptions of these properties,
# see the [configuration documentation](https://docs.redpanda.com/docs/cluster-administration/configuration/).
config:
  cluster: {}
    # auto_create_topics_enabled: true                             # Allow topic auto creation
    # transaction_coordinator_replication: 1                       # Replication factor for a transaction coordinator topic
    # id_allocator_replication: 1                                  # Replication factor for an ID allocator topic
    # disable_metrics: false                                       # Disable registering metrics
    # enable_coproc: false                                         # Enable coprocessing mode
    # enable_idempotence: false                                    # Enable idempotent producer
    # enable_pid_file: true                                        # Enable pid file; You probably don't want to change this
    # enable_transactions: false                                   # Enable transactions
    # group_max_session_timeout_ms: 300s                           # The maximum allowed session timeout for registered consumers; Longer timeouts give consumers more time to process messages in between heartbeats at the cost of a longer time to detect failures; Default quota tracking window size in milliseconds
    # group_min_session_timeout_ms: Optional                       # The minimum allowed session timeout for registered consumers; Shorter timeouts result in quicker failure detection at the cost of more frequent consumer heartbeating
    # kafka_group_recovery_timeout_ms: 30000ms                     # Kafka group recovery timeout expressed in milliseconds
    # kafka_qdc_enable: false                                      # Enable kafka queue depth control
    # kafka_qdc_max_latency_ms: 80ms                               # Max latency threshold for kafka queue depth control depth tracking
    # log_cleanup_policy: deletion                                 # Default topic cleanup policy
    # log_compaction_interval_ms: 5min                             # How often do we trigger background compaction
    # log_compression_type: producer                               # Default topic compression type
    # log_message_timestamp_type: create_time                      # Default topic messages timestamp type
    # retention_bytes: None                                        # max bytes per partition on disk before triggering a compaction
    # rm_sync_timeout_ms: 2000ms
    # rm_violation_recovery_policy: crash                          # Describes how to recover from an invariant violation happened on the partition level
    # target_quota_byte_rate: 2GB                                  # Target quota byte rate in bytes per second
    # tm_sync_timeout_ms: 2000ms                                   # Time to wait state catch up before rejecting a request
    # tm_violation_recovery_policy: crash                          # Describes how to recover from an invariant violation happened on the transaction coordinator level
    # transactional_id_expiration_ms: 10080min                     # Producer ids are expired once this time has elapsed after the last write with the given producer ID
  # -- Tunable cluster properties.
  tunable:
    # -- See the [property reference documentation](https://docs.redpanda.com/docs/reference/tunable-properties/#log_segment_size).
    log_segment_size: 134217728                                    # 128 mb
    # -- See the [property reference documentation](https://docs.redpanda.com/docs/reference/tunable-properties/#log_segment_size_min).
    log_segment_size_min: 16777216                                 # 16 mb
    # -- See the [property reference documentation](https://docs.redpanda.com/docs/reference/tunable-properties/#log_segment_size_max).
    log_segment_size_max: 268435456                                # 256 mb
    # -- See the [property reference documentation](https://docs.redpanda.com/docs/reference/tunable-properties/#kafka_batch_max_bytes).
    kafka_batch_max_bytes: 1048576                                 # 1 mb
    # -- See the [property reference documentation](https://docs.redpanda.com/docs/reference/tunable-properties/#topic_partitions_per_shard).
    topic_partitions_per_shard: 1000
    # -- See the [property reference documentation](https://docs.redpanda.com/docs/reference/tunable-properties/#compacted_log_segment_size).
    compacted_log_segment_size: 67108864                           # 64 mb
    # -- See the [property reference documentation](https://docs.redpanda.com/docs/reference/tunable-properties/#max_compacted_log_segment_size).
    max_compacted_log_segment_size: 536870912                      # 512 mb
    # -- See the [property reference documentation](https://docs.redpanda.com/docs/reference/cluster-properties/#kafka_connection_rate_limit).
    kafka_connection_rate_limit: 1000
    # -- See the [property reference documentation](https://docs.redpanda.com/docs/reference/tunable-properties/#group_topic_partitions).
    group_topic_partitions: 16
    # cloud_storage_enable_remote_read: true                       # cluster wide configuration for read from remote cloud storage
    # cloud_storage_enable_remote_write: true                      # cluster wide configuration for writing to remote cloud storage

    # alter_topic_cfg_timeout_ms: 5s                               # Time to wait for entries replication in controller log when executing alter configuration request
    # compacted_log_segment_size: 256MiB                           # How large in bytes should each compacted log segment be (default 256MiB)
    # controller_backend_housekeeping_interval_ms: 1s              # Interval between iterations of controller backend housekeeping loop
    # coproc_max_batch_size: 32kb                                  # Maximum amount of bytes to read from one topic read
    # coproc_max_inflight_bytes: 10MB                              # Maximum amountt of inflight bytes when sending data to wasm engine
    # coproc_max_ingest_bytes: 640kb                               # Maximum amount of data to hold from input logs in memory
    # coproc_offset_flush_interval_ms: 300000ms                    # Interval for which all coprocessor offsets are flushed to disk
    # create_topic_timeout_ms: 2000ms                              # Timeout (ms) to wait for new topic creation
    # default_num_windows: 10                                      # Default number of quota tracking windows
    # default_window_sec: 1000ms                                   # Default quota tracking window size in milliseconds
    # delete_retention_ms: 10080min                                # delete segments older than this (default 1 week)
    # disable_batch_cache: false                                   # Disable batch cache in log manager
    # fetch_reads_debounce_timeout: 1ms                            # Time to wait for next read in fetch request when requested min bytes wasn't reached
    # fetch_session_eviction_timeout_ms: 60s                       # Minimum time before which unused session will get evicted from sessions; Maximum time after which inactive session will be deleted is two time given configuration valuecache
    # group_initial_rebalance_delay: 300ms                         # Extra delay (ms) added to rebalance phase to wait for new members
    # group_new_member_join_timeout: 30000ms                       # Timeout for new member joins
    # group_topic_partitions: 1                                    # Number of partitions in the internal group membership topic
    # id_allocator_batch_size: 1000                                # ID allocator allocates messages in batches (each batch is a one log record) and then serves requests from memory without touching the log until the batch is exhausted
    # id_allocator_log_capacity: 100                               # Capacity of the id_allocator log in number of messages; Once it reached id_allocator_stm should compact the log
    # join_retry_timeout_ms: 5s                                    # Time between cluster join retries in milliseconds
    # kafka_qdc_idle_depth: 10                                     # Queue depth when idleness is detected in kafka queue depth control
    # kafka_qdc_latency_alpha: 0.002                               # Smoothing parameter for kafka queue depth control latency tracking
    # kafka_qdc_max_depth: 100                                     # Maximum queue depth used in kafka queue depth control
    # kafka_qdc_min_depth: 1                                       # Minimum queue depth used in kafka queue depth control
    # kafka_qdc_window_count: 12                                   # Number of windows used in kafka queue depth control latency tracking
    # kafka_qdc_window_size_ms: 1500ms                             # Window size for kafka queue depth control latency tracking
    # kvstore_flush_interval: 10ms                                 # Key-value store flush interval (ms)
    # kvstore_max_segment_size: 16MB                               # Key-value maximum segment size (bytes)
    # log_segment_size: 1GB                                        # How large in bytes should each log segment be (default 1G)
    # max_compacted_log_segment_size: 5GB                          # Max compacted segment size after consolidation
    # max_kafka_throttle_delay_ms: 60000ms                         # Fail-safe maximum throttle delay on kafka requests
    # metadata_dissemination_interval_ms: 3000ms                   # Interaval for metadata dissemination batching
    # metadata_dissemination_retries: 10                           # Number of attempts of looking up a topic's meta data like shard before failing a request
    # metadata_dissemination_retry_delay_ms: 500ms                 # Delay before retry a topic lookup in a shard or other meta tables
    # quota_manager_gc_sec: 30000ms                                # Quota manager GC frequency in milliseconds
    # raft_learner_recovery_rate: 104857600                        # Raft learner recovery rate in bytes per second
    # raft_heartbeat_disconnect_failures: 3                        # After how many failed heartbeats to forcibly close an unresponsive TCP connection. Set to 0 to disable force disconnection.
    # raft_heartbeat_interval_ms: 150                              # The interval in ms between raft leader heartbeats.
    # raft_heartbeat_timeout_ms: 3000                              # Raft heartbeat RPC timeout.
    # raft_io_timeout_ms: 10000                                    # Raft I/O timeout.
    # raft_max_concurrent_append_requests_per_follower: 16         # Maximum number of concurrent append entries requests sent by leader to one follower.
    # raft_max_recovery_memory: 33554432                           # Maximum memory that can be used for reads in the raft recovery process.
    # raft_recovery_default_read_size: 524288                      # Default size of read issued during raft follower recovery.
    # raft_replicate_batch_window_size: 1048576                    # Maximum size of requests cached for replication.
    # raft_smp_max_non_local_requests:                             # Maximum number of x-core requests pending in Raft seastar::smp group. (for more details look at seastar::smp_service_group documentation).
    # raft_timeout_now_timeout_ms: 1000                            # Timeout for a timeout now request.
    # raft_transfer_leader_recovery_timeout_ms: 1000               # Timeout waiting for follower recovery when transferring leadership.
    # raft_election_timeout_ms: 1500ms                             # Election timeout expressed in milliseconds TBD - election_time_out
    # readers_cache_eviction_timeout_ms: 30s                       # Duration after which inactive readers will be evicted from cache
    # reclaim_growth_window: 3000ms                                # Length of time in which reclaim sizes grow
    # reclaim_max_size: 4MB                                        # Maximum batch cache reclaim size
    # reclaim_min_size: 128KB                                      # Minimum batch cache reclaim size
    # reclaim_stable_window: 10000ms                               # Length of time above which growth is reset
    # recovery_append_timeout_ms: 5s                               # Timeout for append entries requests issued while updating stale follower
    # release_cache_on_segment_roll: false                         # Free cache when segments roll
    # replicate_append_timeout_ms: 3s                              # Timeout for append entries requests issued while replicating entries
    # segment_appender_flush_timeout_ms: 1ms                       # Maximum delay until buffered data is written
    # wait_for_leader_timeout_ms: 5000ms                           # Timeout (ms) to wait for leadership in metadata cache
  # -- Node (broker) properties.
  # See the [property reference documentation](https://docs.redpanda.com/docs/reference/node-properties/).
  node: {}
    # node_id:                                                     # Unique ID identifying a node in the cluster
    # data_directory:                                              # Place where redpanda will keep the data
    # admin_api_doc_dir: /usr/share/redpanda/admin-api-doc         # Admin API doc directory
    # api_doc_dir: /usr/share/redpanda/proxy-api-doc               # API doc directory
    # coproc_supervisor_server: 127.0.0.1:43189                    # IpAddress and port for supervisor service
    # dashboard_dir: None                                          # serve http dashboard on / url
    # developer_mode: optional                                     # Skips most of the checks performed at startup

  # Invalid properties
  # Any of these properties will be ignored. These otherwise valid properties are not allowed
  # to be used in this section since they impact deploying Redpanda in Kubernetes.
  # Make use of the above sections to modify these values instead (see comments below).
  # admin: "127.0.0.1:9644"                                        # Address and port of admin server
  # admin_api_tls: validate_many                                   # TLS configuration for admin HTTP server
  # advertised_kafka_api: None                                     # Address of Kafka API published to the clients
  # advertised_pandaproxy_api: None                                # Rest API address and port to publish to client
  # advertised_rpc_api: None                                       # Address of RPC endpoint published to other cluster members
  # default_topic_partitions: 1                                    # Default number of partitions per topic
  # default_topic_replications: 3                                  # Default replication factor for new topics
  # enable_admin_api: true                                         # Enable the admin API
  # enable_sasl: false                                             # Enable SASL authentication for Kafka connections
  # kafka_api: "127.0.0.1:9092"                                    # Address and port of an interface to listen for Kafka API requests
  # kafka_api_tls: None                                            # TLS configuration for Kafka API endpoint
  # pandaproxy_api: "0.0.0.0:8082"                                 # Rest API listen address and port
  # pandaproxy_api_tls: validate_many                              # TLS configuration for Pandaproxy api
  # rpc_server: "127.0.0.1:33145"                                  # IP address and port for RPC server
  # rpc_server_tls: validate                                       # TLS configuration for RPC server
  # seed_servers: None                                             # List of the seed servers used to join current cluster; If the seed_server list is empty the node will be a cluster root and it will form a new cluster
  # superusers: None                                               # List of superuser usernames
