# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# This file contains values for variables referenced from yaml files in the templates directory.
#
# For further information on Helm templating see the documentation at:
#  https://helm.sh/docs/chart_template_guide/values_files/

#
# >>> This chart requires Helm version 3.6.0 or greater <<<
#

# Common parameters
#
# Override redpanda.name template
nameOverride: ""
# Override redpanda.fullname template
fullnameOverride: ""
# Default kubernetes cluster domain
clusterDomain: cluster.local
# Additional labels added to all Kubernetes objects
# e.g., my.k8s.service: redpanda
commonLabels: {}
# Node selection constraints for scheduling Pods, can override this for statefulsets.
# https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector
nodeSelector: {}
# Taints to be tolerated by Pods, can override this for statefulsets.
# https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
tolerations: []

# Redpanda parameters
#
image:
  repository: vectorized/redpanda
  # Redpanda version defaults to Chart.appVersion
  tag: ""
  # The imagePullPolicy will default to Always when the tag is 'latest'
  pullPolicy: IfNotPresent
# Your license key (optional)
license_key: ""
license_secret_ref: {}
# Secret name and secret key where license is stored
# secret_name: my-secret
# secret_key: key-where-license-is-stored
# Rack Awareness
rackAwareness:
  # When running in multiple racks or availability zones, use the Kubernetes Node
  # annotation value as the Redpanda rack value.
  # Enabling this requires running with a service account with "get" Node permissions.
  # Set serviceAccount.create=true and rbac.enabled=true to have the helm chart configure that permission.
  enabled: false
  # This is the common well-known annotation to use. You would only want to override
  # this if you have your own custom Node annotation to use instead.
  nodeAnnotation: topology.kubernetes.io/zone

#
# Console
console:
  enabled: true
  configmap:
    create: false
  secret:
    create: false
  deployment:
    create: false
  config: {}

#
# Authentication
auth:
  # SASL configuration
  sasl:
    # When enabling SASL, you are required to have a secret available and referenced in secretRef
    enabled: false
    # if unspecified, mechanism defaults to SCRAM-SHA-512
    mechanism: SCRAM-SHA-512
    # A secret is expected to exist to contain a file with users in the following format:
    # <userName>:<password>:<mechanism>
    # You can create these users in the following way:
    # create a file with one entry per line in the following format:
    # <username>:<password>:<mechanism>
    # Ensure there is an empty line at the end of the file.
    # kubectl -n redpanda create secret generic my-users --from-file=users.txt
    # Then secretRef is required, regardless if created a users list (below) or not
    secretRef: "redpanda-users"
    # optional list of users
    # If not an empty list, these users will be created in a secret whose name is defined in "secretRef"
    users:
    - name: admin
      password: change-me
      mechanism: SCRAM-SHA-512

# TLS configuration
tls:
  # Enable global TLS, which turns on TLS by default for all listeners
  # Each listener must include a certificate name in its TLS section
  # Any certificates in auth.tls.certs will still be loaded if enabled is false
  # This is because listeners may enable TLS individually (see listeners.<listener name>.tls.enabled)
  enabled: false
  # list all certificates below, then reference a certificate's name in each listener (see listeners.<listener name>.tls.cert)
  certs:
    # This is the certificate name that is used to associate the certificate with a listener
    # See listeners.<listener group>.tls.cert for more information
    default:
      # Define an issuerRef to use your own custom pre-installed Issuer
      # issuerRef:
      #   name: redpanda-default-root-issuer
      #   kind: Issuer   # Can be Issuer or ClusterIssuer
      # The caEnabled flag determines whether the ca.crt file is included in the TLS mount path on each Redpanda pod
      caEnabled: true
      # duration: 43800h

#
# External access configuration
external:
  # Default external access value for each service
  # Services can toggle external access per listener (ie. `listeners.<service name>.external.<listener name>.enabled`)
  enabled: true
  # External access type. `NodePort` is the only acceptable value if defined.
  # If undefined, then advertised listeners will be configured in Redpanda, but the helm chart will not create a Service. You must create a Service manually.
  type: NodePort
  # Optional domain advertised to external clients
  # If specified, then it will be appended to the `external.addresses` values as each broker's advertised address
  # domain: local
  # An optional addresses list with an entry for each broker / statefulset replica combo (default count of 3)
  # The values can be IP addresses or DNS names. Keep in mind that `external.domain`, if exists, will be appended to this value
  # addresses:
  # - redpanda-0
  # - redpanda-1
  # - redpanda-2
  #
  # annotations:
    # For example:
    # cloud.google.com/load-balancer-type: "Internal"
    # service.beta.kubernetes.io/aws-load-balancer-type: nlb

# Logging
logging:
  # Log level
  # Valid values (from least to most logging) are warn, info, debug, trace
  logLevel: info
  #
  # Send usage stats back to Redpanda
  # See https://docs.redpanda.com/docs/cluster-administration/monitoring/#stats-reporting
  usageStats:
    # rpk.enable_usage_stats
    enabled: true
    # Your organization name (optional)
    # organization: your-org
    # Your cluster ID (optional)
    # clusterId: your-helm-cluster
#
resources:
  # Both Redpanda and Kubernetes have multiple ways to allocate resources.
  # There are also several associated parameters that impact how these resources are used by
  # Kubernetes, the Redpanda app, and the subsystem Redpanda is built on (Seastar).
  # This section attempts to simplify allocating resources by providing a single location
  # where resources are defined.
  # Helm sets these resource values within the following templates:
  # - statefulset.yaml
  # - configmap.yaml
  #
  # The default values below are what should work for a development environment.
  # Production-level values and other considerations are provided in comments
  # if those values are different from the default.
  #
  cpu:
    # Redpanda makes use of a thread per core model described here:
    # https://redpanda.com/blog/tpc-buffers
    # For this reason, Redpanda should only be given full cores (cores parameter below).
    #
    # NOTE: You can increase cores, but decreasing cores is not currently supported:
    # https://github.com/redpanda-data/redpanda/issues/350
    #
    # Equivalent to: --smp, resources.requests.cpu, and resources.limits.cpu
    # For production: 4 or greater
    cores: 1
    #
    # Overprovisioned means Redpanda won't assume it has all of the provisioned CPU.
    # This should be true unless the container has CPU affinity (eg. min and max above are equal).
    # Equivalent to: --idle-poll-time-us 0 --thread-affinity 0 --poll-aio 0
    # overprovisioned: false
  #
  memory:
    # Enables memory locking.
    # For production: true
    # enable_memory_locking: false
    #
    # It is recommended to have at least 2Gi of memory per core for the Redpanda binary.
    # This memory is taken from the total memory given to each container.
    # We allocate 80% of the container's memory to Redpanda, leaving the rest for
    # the Seastar subsystem (reserveMemory) and other container processes.
    # So at least 2.5Gi per core is recommended in order to ensure Redpanda has a full 2Gi.
    #
    # These values affect --memory and --reserve-memory flags passed to Redpanda and the memory
    # requests/limits in the StatefulSet.
    # Valid suffixes: k M G Ki Mi Gi
    # Only support a single decimal (eg. 2.5Gi rather than 2.55Gi)
    #
    container:
      # Minimum memory count for each Redpanda broker
      # If omitted, the min value will equal the max value (requested resources defaults to limits)
      # Equivalent to: resources.requests.memory
      # For production: 10Gi or greater
      # min: 2.5Gi
      #
      # Minimum memory count for each Redpanda broker
      # Equivalent to: resources.limits.memory
      # For production: 10Gi or greater
      max: 2.5Gi
    #
    # redpanda:
      # This optional redpanda section allows specifying the memory size for both the Redpanda
      # process and the underlying reserved memory (used by Seastar).
      # This section is omitted by default, and memory sizes are calculated automatically
      # based on container memory.
      # Uncommenting this section and setting memory and reserveMemory values will disable
      # automatic calculation.
      #
      # If you are setting the following values manually, keep in mind the following guidelines (getting
      # this wrong will potentially lead to performance issues, instability, loss of data, etc.):
      # The amount of memory to allocate to a container is determined by the sum of three values:
      # 1. Redpanda (at least 2Gi per core, ~80% of the container's total memory)
      # 2. Seastar subsystem (200Mi * 0.2% of the container's total memory, 200Mi < x < 1Gi)
      # 3. other container processes (whatever small amount remains)
      #
      # Memory for the Redpanda process.
      # This must be lower the container's memory (resources.memory.container.min if provided, otherwise
      # resources.memory.container.max).
      # Equivalent to: --memory
      # For production: 8Gi or greater
      # memory: 2Gi
      #
      # Memory reserved for the Seastar subsystem.
      # Any value above 1Gi will provide diminishing performance benefits.
      # Equivalent to: --reserve-memory
      # For production: 1Gi
      # reserveMemory: 200Mi
#
# Persistence
storage:
  # Absolute path on host to store Redpanda's data.
  # If not specified, then `emptyDir` will be used instead.
  # If specified, but `persistentVolume.enabled` is `true`, then has no effect.
  hostPath: ""
  # Absolute path on host to store Redpanda's tiered storage cache data.
  # If not specified, then `emptyDir` will be used instead.
  # If specified, but `tieredStoragePersistentVolume.enabled` is `true`, then has no effect.
  tieredStorageHostPath: ""
  # If `enabled` is `true` then a PersistentVolumeClaim will be created for tiered storage cache and
  # used to store data retrieved from cloud storage (eg. S3), otherwise `hostPath` is used.
  tieredStoragePersistentVolume:
    enabled: false
    # If defined, then `storageClassName: <storageClass>`.
    # If set to "-", then `storageClassName: ""`, which disables dynamic
    # provisioning.
    # If undefined or empty (default), then no `storageClassName` spec is set,
    # so the default provisioner will be chosen (gp2 on AWS, standard on
    # GKE, AWS & OpenStack).
    storageClass: ""
    # Additional labels to apply to the created PersistentVolumeClaims.
    labels: {}
    # Additional annotations to apply to the created PersistentVolumeClaims.
    annotations: {}
  # If `enabled` is `true` then a PersistentVolumeClaim will be created and
  # used to store Redpanda's data, otherwise `hostPath` is used.
  persistentVolume:
    enabled: true
    size: 20Gi
    # If defined, then `storageClassName: <storageClass>`.
    # If set to "-", then `storageClassName: ""`, which disables dynamic
    # provisioning.
    # If undefined or empty (default), then no `storageClassName` spec is set,
    # so the default provisioner will be chosen (gp2 on AWS, standard on
    # GKE, AWS & OpenStack).
    storageClass: ""
    # Additional labels to apply to the created PersistentVolumeClaims.
    labels: {}
    # Additional annotations to apply to the created PersistentVolumeClaims.
    annotations: {}
  # Tiered Storage helps to lower storage costs by offloading log segments to cloud storage.
  # You can specify the amount of local storage you want to retain in local storage.
  # You don't need to specify which log segments you want to move because Redpanda moves them automatically
  # based on cluster-level configuration properties. Redpanda Tiered Storage indexes where data is offloaded,
  # so it can retrieve the data when you need it.
  #
  # Prerequisite:
  # * AWS S3 bucket or GCP bucket or Azure blob storage
  # * IAM role that has access to bucket
  #
  # REF: https://docs.redpanda.com/docs/manage/tiered-storage/#tiered-storage-configuration-properties
  #
  # Topic configuration flags override any cluster-level configuration. For example, on new topics,
  # if `cloud_storage_enable_remote_write` is set to `true`, you can set `redpanda.remote.write` to `false`
  # to turn it off for a particular topic. Or, if you have Tiered Storage turned off for the cluster, you can enable it
  # for individual topics with the topic flags. When cluster-level properties are changed,
  # the changes apply only to new topics, not existing topics.
  #
  # REF: https://docs.redpanda.com/docs/manage/tiered-storage/#enable-tiered-storage-for-a-topic
  tieredConfig:
    # Global flag that enables Tiered Storage feature if license is provided.
    # Default is false.
    cloud_storage_enabled: false
    # Cluster wide config option to enable data upload from Redpanda to cloud storage if set to true.
    cloud_storage_enable_remote_write: true
    # Cluster wide config option to enable Redpanda to fetch data from cloud storage if set to true.
    cloud_storage_enable_remote_read: true

    # `cloud_storage_credentials_source` configuration option is available starting from 22.3.X
    #
    # Possible values:
    # `config_file` - If IAM roles are not available, specify credentials in the cluster configuration file.
    # `aws_instance_metadata` - For an AWS EC2 instance, use the instance metadata API from AWS.
    # `sts` - For AWS on Kubernetes, use the Secure Token Service (STS).
    # `gcp_instance_metadata` - For a VM running on GCP, or for Google Kubernetes Engine (GKE), use the instance metadata API from GCP.
    #
    # REF: https://docs.redpanda.com/docs/manage/security/iam-roles/#configuring-iam-roles
    cloud_storage_credentials_source: config_file

    # Cloud storage region. Required.
    cloud_storage_region: ""
    # Cloud storage bucket name. Required.
    cloud_storage_bucket: ""
    # Cloud storage access key used to authenticated if `cloud_storage_credentials_source` is set to `config`
    # cloud_storage_access_key: ""
    # Cloud storage secret key used to authenticated if `cloud_storage_credentials_source` is set to `config`
    # cloud_storage_secret_key: ""
    # API endpoint.
    #
    # - For AWS, this can be left blank. It’s generated automatically using the region and bucket.
    #
    # - For Google Cloud Service, use storage.googleapis.com.
    # cloud_storage_api_endpoint: ""

    # Maximum size of the disk cache used by Tiered Storage.
    # Default is 20 GiB.
    cloud_storage_cache_size: 21474836480
    # The directory for the Tiered Storage cache. You must specify the full path. Default is: <redpanda-data-directory>/cloud_storage_cache.
    # cloud_storage_cache_directory: ""

    # The time, in milliseconds, between cache checks. The size of the cache can grow quickly,
    # so it’s important to have a small interval between checks, but if the checks are too frequent,
    # they consume a lot of resources.
    # Default is 30000 ms.
    # cloud_storage_cache_check_interval: 30000

    # The time, in milliseconds, that is used as an initial backoff interval in the exponential
    # backoff algorithm to handle an error.
    # Default is 100 ms.
    # cloud_storage_initial_backoff_ms: 100

    # The maximum number of connections to cloud storage on a node per CPU.
    # Remote read and remote write share the same pool of connections.
    # This means that if a connection is being used to upload a segment, it cannot be used to download another segment.
    # If this value is too small, some workloads might starve for connections, which results in delayed uploads and downloads.
    # If this value is too large, Redpanda tries to upload a lot of files at the same time and might overwhelm the system.
    # Default is 20.
    # cloud_storage_max_connections: 20

    # Timeout for segment upload. Redpanda retries the upload after the timeout.
    # Default is 30000 ms.
    # cloud_storage_segment_upload_timeout_ms: 30000

    # Timeout for manifest upload. Redpanda retries the upload after the timeout.
    # Default is 10000 ms.
    # cloud_storage_manifest_upload_timeout_ms: 10000

    # The maximum idle time for persistent HTTP connections. Differs depending on the cloud provider.
    # Default is 5000 ms, which is sufficient for most providers.
    # cloud_storage_max_connection_idle_time_ms: 5000

    # Sets the number of seconds for idle timeout. If this property is empty, Redpanda uploads metadata to the cloud storage,
    # but the segment is not uploaded until it reaches the segment.bytes size.
    # By default, the property is empty.
    # cloud_storage_segment_max_upload_interval_sec: 1
    # The public certificate used to validate the TLS connection to cloud storage. If this is empty, Redpanda uses your operating system's CA cert pool.
    # cloud_storage_trust_file: ""

    # Under normal circumstances, you should not need to configure the following properties:

    # The recompute interval for the upload controller. Default is 60000 ms.
    # cloud_storage_upload_ctrl_update_interval_ms: 60000

    # The proportional coefficient for the upload controller. Default is -2.
    # cloud_storage_upload_ctrl_p_coeff: -2

    # The derivative coefficient for the upload controller. Default is 0.
    # cloud_storage_upload_ctrl_d_coeff: 0

    # The minimum number of I/O and CPU shares that the remote write process can use. Default is 100.
    # cloud_storage_upload_ctrl_min_shares: 100

    # The maximum number of I/O and CPU shares that the remote write process can use. Default is 1000.
    # cloud_storage_upload_ctrl_max_shares: 1000

    # Sets the interval, in milliseconds, used to reconcile partitions that need to be uploaded.
    # A long reconciliation interval can result in a delayed reaction to topic creation,
    # topic deletion, or leadership rebalancing events. A short reconciliation interval
    # guarantees that new partitions are picked up quickly, but the process uses more resources.
    # Default is 10000 ms.
    # cloud_storage_reconciliation_interval_ms: 10000

    # Disables TLS encryption. You can set this to true if TLS termination is done by the proxy, such as HAProxy. Default is false.
    # cloud_storage_disable_tls: false

    # Overrides the default API endpoint port. Default is 443.
    # cloud_storage_api_endpoint_port: 443

post_install_job:
  enabled: true
  # Resource requests and limits for the post-install batch job
  # resources:
  #   requests:
  #     cpu: 1
  #     memory: 512Mi
  #   limits:
  #     cpu: 2
  #     memory: 1024Mi
  # labels: {}
  # annotations: {}

post_upgrade_job:
  enabled: true
  # Resource requests and limits for the post-upgrade batch job
  # resources:
  #   requests:
  #     cpu: 1
  #     memory: 512Mi
  #   limits:
  #     cpu: 2
  #     memory: 1024Mi
  # labels: {}
  # annotations: {}
  # Additional environment variables for the Post Upgrade Job
  # extraEnv:
  #   - name: AWS_SECRET_ACCESS_KEY
  #     valueFrom:
  #       secretKeyRef:
  #         name: my-secret
  #         key: redpanda-aws-secret-access-key
  # Additional environment variables for the Post Upgrade Job mapped from Secret or ConfigMap
  # extraEnvFrom:
  #   - secretRef:
  #       name: redpanda-aws-secrets

statefulset:
  # Number of Redpanda brokers (recommend setting this to the number of nodes in the cluster)
  replicas: 3
  updateStrategy:
    type: RollingUpdate
  budget:
    maxUnavailable: 1
  # Additional annotations to apply to the Pods of this StatefulSet.
  annotations: {}
  # Adjust the period for your probes to meet your needs (see https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes)
  startupProbe:
    initialDelaySeconds: 1
    failureThreshold: 120
    periodSeconds: 10
  livenessProbe:
    initialDelaySeconds: 10
    failureThreshold: 3
    periodSeconds: 10
  readinessProbe:
    initialDelaySeconds: 1
    failureThreshold: 3
    periodSeconds: 10
    successThreshold: 1
  #
  # A note regarding statefulset resources:
  # Resources are set through the top-level resources section above.
  # It is recommended to set resources values in that section rather than here, as this will guarantee
  # memory is allocated across containers, Redpanda, and the Seastar subsystem correctly.
  # This automatic memory allocation is in place because Repanda and the Seastar subsystem require flags
  # at startup that set the amount of memory available to each process.
  # Kubernetes (mainly statefulset), Redpanda, and Seastar memory values are tightly coupled.
  # Adding a resource section here will be ignored.
  #
  # Inter-Pod Affinity rules for scheduling Pods of this StatefulSet.
  # https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  podAffinity: {}
  # Anti-affinity rules for scheduling Pods of this StatefulSet.
  # https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  # You may either toggle options below for default anti-affinity rules,
  # or specify the whole set of anti-affinity rules instead of them.
  podAntiAffinity:
    # The topologyKey to be used.
    # Can be used to spread across different nodes, AZs, regions etc.
    topologyKey: kubernetes.io/hostname
    # Type of anti-affinity rules: either `soft`, `hard` or empty value (which
    # disables anti-affinity rules).
    type: hard
    # Weight for `soft` anti-affinity rules.
    # Does not apply for other anti-affinity types.
    weight: 100
  # Node selection constraints for scheduling Pods of this StatefulSet.
  # these override the global nodeSelector value
  # https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector
  nodeSelector: {}
  # PriorityClassName given to Pods of this StatefulSet
  # https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/#priorityclass
  priorityClassName: ""
  # Taints to be tolerated by Pods of this StatefulSet.
  # these override the global tolerations value
  # https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  tolerations: []
  # https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/
  topologySpreadConstraints:
    maxSkew: 1
    topologyKey: topology.kubernetes.io/zone
    whenUnsatisfiable: ScheduleAnyway
  securityContext:
    fsGroup: 101
    runAsUser: 101
  initContainerImage:
    repository: busybox
    tag: latest

# Service account management
serviceAccount:
  # Specifies whether a service account should be created
  create: false
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""
# Role Based Access Control
rbac:
  # enable this for features that need extra priveleges
  enabled: false
  # annotations to add to the rbac resources
  annotations: {}

tuning: {}
  # This section contains Redpanda tuning parameters.
  # Each parameter below is set to their default values.
  # Remove the curly brackets above if you uncomment any parameters below.
  #
  # Increases the number of allowed asynchronous IO events.
  # tune_aio_events: false
  #
  # Syncs NTP
  # tune_clocksource: false
  #
  # Creates a "ballast" file so that, if a Redpanda node runs out of space,
  # you can delete the ballast file to allow the node to resume operations and then
  # delete a topic or records to reduce the space used by Redpanda.
  # tune_ballast_file: false
  #
  # The path where the ballast file will be created.
  # ballast_file_path: "/var/lib/redpanda/data/ballast"
  #
  # The ballast file size.
  # ballast_file_size: "1GiB"
  #
  # (Optional) The vendor, VM type and storage device type that redpanda will run on, in
  # the format <vendor>:<vm>:<storage>. This hints to rpk which configuration values it
  # should use for the redpanda IO scheduler.
  # Some valid values are "gcp:c2-standard-16:nvme", "aws:i3.xlarge:default"
  # well_known_io: ""
  #
  # The following tuning parameters must be false in container environments and will be ignored:
  #   tune_network
  #   tune_disk_scheduler
  #   tune_disk_nomerges
  #   tune_disk_irq
  #   tune_fstrim
  #   tune_cpu
  #   tune_swappiness
  #   tune_transparent_hugepages
  #   tune_coredump

### Overrides
#
# This sections can be used to override global settings configured above for individual
# listeners.
#
listeners:
  # Admin API listener (only one)
  admin:
    # The port for both internal and external connections to the admin API
    port: 9644
    # Optional external section
    external:
      # Name of the external listener
      default:
        # This overrides `external.enabled` for only this listener
        # enabled: true
        # The port advertised to this listener's external clients
        # List one port if you want to use the same port for each broker (would be the case when using NodePort service)
        # Otherwise list the port you want to use for each broker in order by statefulset replica
        # If undefined, `listeners.admin.port` is used
        advertisedPorts:
        - 31644
    # Optional TLS section (required if global TLS is enabled)
    tls:
      # Optional flag to override the global TLS enabled flag
      # enabled: true
      # Name of certificate used for TLS (must match a cert registered at tls.certs)
      cert: default
      # If true, the truststore file for this listener will be included in the ConfigMap
      requireClientAuth: false
  # Kafka API listeners
  kafka:
    # The port for internal client connections (10092 is default to avoid conflicts with the external default port 9092)
    port: 9093
    tls:
      # enabled: true
      cert: default
      requireClientAuth: false
    external:
      default:
        # enabled: true
        # The port used for external client connections
        port: 9094
        # If undefined, `listeners.kafka.external.port` is used
        advertisedPorts:
        - 31092
  # HTTP API listeners (aka PandaProxy)
  http:
    enabled: true
    port: 8082
    kafkaEndpoint: default
    tls:
      # enabled: true
      cert: default
      requireClientAuth: false
    external:
      default:
        # enabled: true
        port: 8083
        advertisedPorts:
        - 30082
  # RPC listener (this is never externally accessible)
  rpc:
    port: 33145
    tls:
      # enabled: true
      cert: default
      requireClientAuth: false
  # Schema registry listeners
  schemaRegistry:
    enabled: true
    port: 8081
    kafkaEndpoint: default
    tls:
      # enabled: true
      cert: default
      requireClientAuth: false
    external:
      default:
        # enabled: true
        port: 8084
        advertisedPorts:
        - 30081

# Expert Config

# This section contains various settings supported by Redpanda that may not work
# correctly in a kubernetes cluster. Changing these settings comes with some risk.
#
# Here be dragons!
#
# This section allows modifying various Redpanda settings not covered in other sections above.
# These values do not pertain to the kubernetes objects created with helm.
# Instead these parameters get passed directly to the Redpanda binary at startup.
# See https://docs.redpanda.com/docs/cluster-administration/configuration/
config:
  cluster: {}
    # auto_create_topics_enabled: true                             # Allow topic auto creation
    # transaction_coordinator_replication: 1                       # Replication factor for a transaction coordinator topic
    # id_allocator_replication: 1                                  # Replication factor for an ID allocator topic
    # disable_metrics: false                                       # Disable registering metrics
    # enable_coproc: false                                         # Enable coprocessing mode
    # enable_idempotence: false                                    # Enable idempotent producer
    # enable_pid_file: true                                        # Enable pid file; You probably don't want to change this
    # enable_transactions: false                                   # Enable transactions
    # group_max_session_timeout_ms: 300s                           # The maximum allowed session timeout for registered consumers; Longer timeouts give consumers more time to process messages in between heartbeats at the cost of a longer time to detect failures; Default quota tracking window size in milliseconds
    # group_min_session_timeout_ms: Optional                       # The minimum allowed session timeout for registered consumers; Shorter timeouts result in quicker failure detection at the cost of more frequent consumer heartbeating
    # kafka_group_recovery_timeout_ms: 30000ms                     # Kafka group recovery timeout expressed in milliseconds
    # kafka_qdc_enable: false                                      # Enable kafka queue depth control
    # kafka_qdc_max_latency_ms: 80ms                               # Max latency threshold for kafka queue depth control depth tracking
    # log_cleanup_policy: deletion                                 # Default topic cleanup policy
    # log_compaction_interval_ms: 5min                             # How often do we trigger background compaction
    # log_compression_type: producer                               # Default topic compression type
    # log_message_timestamp_type: create_time                      # Default topic messages timestamp type
    # retention_bytes: None                                        # max bytes per partition on disk before triggering a compaction
    # rm_sync_timeout_ms: 2000ms
    # rm_violation_recovery_policy: crash                          # Describes how to recover from an invariant violation happened on the partition level
    # target_quota_byte_rate: 2GB                                  # Target quota byte rate in bytes per second
    # tm_sync_timeout_ms: 2000ms                                   # Time to wait state catch up before rejecting a request
    # tm_violation_recovery_policy: crash                          # Describes how to recover from an invariant violation happened on the transaction coordinator level
    # transactional_id_expiration_ms: 10080min                     # Producer ids are expired once this time has elapsed after the last write with the given producer ID
  tunable:
    log_segment_size: 134217728                                    # 128 mb
    log_segment_size_min: 16777216                                 # 16 mb
    log_segment_size_max: 268435456                                # 256 mb
    kafka_batch_max_bytes: 1048576                                 # 1 mb
    topic_partitions_per_shard: 1000
    compacted_log_segment_size: 67108864                           # 64 mb
    max_compacted_log_segment_size: 536870912                      # 512 mb
    kafka_connection_rate_limit: 1000
    group_topic_partitions: 16
    # cloud_storage_enable_remote_read: true                       # cluster wide configuration for read from remote cloud storage
    # cloud_storage_enable_remote_write: true                      # cluster wide configuration for writing to remote cloud storage

    # alter_topic_cfg_timeout_ms: 5s                               # Time to wait for entries replication in controller log when executing alter configuration request
    # compacted_log_segment_size: 256MiB                           # How large in bytes should each compacted log segment be (default 256MiB)
    # controller_backend_housekeeping_interval_ms: 1s              # Interval between iterations of controller backend housekeeping loop
    # coproc_max_batch_size: 32kb                                  # Maximum amount of bytes to read from one topic read
    # coproc_max_inflight_bytes: 10MB                              # Maximum amountt of inflight bytes when sending data to wasm engine
    # coproc_max_ingest_bytes: 640kb                               # Maximum amount of data to hold from input logs in memory
    # coproc_offset_flush_interval_ms: 300000ms                    # Interval for which all coprocessor offsets are flushed to disk
    # create_topic_timeout_ms: 2000ms                              # Timeout (ms) to wait for new topic creation
    # default_num_windows: 10                                      # Default number of quota tracking windows
    # default_window_sec: 1000ms                                   # Default quota tracking window size in milliseconds
    # delete_retention_ms: 10080min                                # delete segments older than this (default 1 week)
    # disable_batch_cache: false                                   # Disable batch cache in log manager
    # fetch_reads_debounce_timeout: 1ms                            # Time to wait for next read in fetch request when requested min bytes wasn't reached
    # fetch_session_eviction_timeout_ms: 60s                       # Minimum time before which unused session will get evicted from sessions; Maximum time after which inactive session will be deleted is two time given configuration valuecache
    # group_initial_rebalance_delay: 300ms                         # Extra delay (ms) added to rebalance phase to wait for new members
    # group_new_member_join_timeout: 30000ms                       # Timeout for new member joins
    # group_topic_partitions: 1                                    # Number of partitions in the internal group membership topic
    # id_allocator_batch_size: 1000                                # ID allocator allocates messages in batches (each batch is a one log record) and then serves requests from memory without touching the log until the batch is exhausted
    # id_allocator_log_capacity: 100                               # Capacity of the id_allocator log in number of messages; Once it reached id_allocator_stm should compact the log
    # join_retry_timeout_ms: 5s                                    # Time between cluster join retries in milliseconds
    # kafka_qdc_idle_depth: 10                                     # Queue depth when idleness is detected in kafka queue depth control
    # kafka_qdc_latency_alpha: 0.002                               # Smoothing parameter for kafka queue depth control latency tracking
    # kafka_qdc_max_depth: 100                                     # Maximum queue depth used in kafka queue depth control
    # kafka_qdc_min_depth: 1                                       # Minimum queue depth used in kafka queue depth control
    # kafka_qdc_window_count: 12                                   # Number of windows used in kafka queue depth control latency tracking
    # kafka_qdc_window_size_ms: 1500ms                             # Window size for kafka queue depth control latency tracking
    # kvstore_flush_interval: 10ms                                 # Key-value store flush interval (ms)
    # kvstore_max_segment_size: 16MB                               # Key-value maximum segment size (bytes)
    # log_segment_size: 1GB                                        # How large in bytes should each log segment be (default 1G)
    # max_compacted_log_segment_size: 5GB                          # Max compacted segment size after consolidation
    # max_kafka_throttle_delay_ms: 60000ms                         # Fail-safe maximum throttle delay on kafka requests
    # metadata_dissemination_interval_ms: 3000ms                   # Interaval for metadata dissemination batching
    # metadata_dissemination_retries: 10                           # Number of attempts of looking up a topic's meta data like shard before failing a request
    # metadata_dissemination_retry_delay_ms: 500ms                 # Delay before retry a topic lookup in a shard or other meta tables
    # quota_manager_gc_sec: 30000ms                                # Quota manager GC frequency in milliseconds
    # raft_learner_recovery_rate: 104857600                        # Raft learner recovery rate in bytes per second
    # raft_heartbeat_disconnect_failures: 3                        # After how many failed heartbeats to forcibly close an unresponsive TCP connection. Set to 0 to disable force disconnection.
    # raft_heartbeat_interval_ms: 150                              # The interval in ms between raft leader heartbeats.
    # raft_heartbeat_timeout_ms: 3000                              # Raft heartbeat RPC timeout.
    # raft_io_timeout_ms: 10000                                    # Raft I/O timeout.
    # raft_max_concurrent_append_requests_per_follower: 16         # Maximum number of concurrent append entries requests sent by leader to one follower.
    # raft_max_recovery_memory: 33554432                           # Maximum memory that can be used for reads in the raft recovery process.
    # raft_recovery_default_read_size: 524288                      # Default size of read issued during raft follower recovery.
    # raft_replicate_batch_window_size: 1048576                    # Maximum size of requests cached for replication.
    # raft_smp_max_non_local_requests:                             # Maximum number of x-core requests pending in Raft seastar::smp group. (for more details look at seastar::smp_service_group documentation).
    # raft_timeout_now_timeout_ms: 1000                            # Timeout for a timeout now request.
    # raft_transfer_leader_recovery_timeout_ms: 1000               # Timeout waiting for follower recovery when transferring leadership.
    # raft_election_timeout_ms: 1500ms                             # Election timeout expressed in milliseconds TBD - election_time_out
    # readers_cache_eviction_timeout_ms: 30s                       # Duration after which inactive readers will be evicted from cache
    # reclaim_growth_window: 3000ms                                # Length of time in which reclaim sizes grow
    # reclaim_max_size: 4MB                                        # Maximum batch cache reclaim size
    # reclaim_min_size: 128KB                                      # Minimum batch cache reclaim size
    # reclaim_stable_window: 10000ms                               # Length of time above which growth is reset
    # recovery_append_timeout_ms: 5s                               # Timeout for append entries requests issued while updating stale follower
    # release_cache_on_segment_roll: false                         # Free cache when segments roll
    # replicate_append_timeout_ms: 3s                              # Timeout for append entries requests issued while replicating entries
    # segment_appender_flush_timeout_ms: 1ms                       # Maximum delay until buffered data is written
    # wait_for_leader_timeout_ms: 5000ms                           # Timeout (ms) to wait for leadership in metadata cache
  node: {}
    # node_id:                                                     # Unique ID identifying a node in the cluster
    # data_directory:                                              # Place where redpanda will keep the data
    # admin_api_doc_dir: /usr/share/redpanda/admin-api-doc         # Admin API doc directory
    # api_doc_dir: /usr/share/redpanda/proxy-api-doc               # API doc directory
    # coproc_supervisor_server: 127.0.0.1:43189                    # IpAddress and port for supervisor service
    # dashboard_dir: None                                          # serve http dashboard on / url
    # developer_mode: optional                                     # Skips most of the checks performed at startup

  # Invalid properties
  # Any of these properties will be ignored. These otherwise valid properties are not allowed
  # to be used in this section since they impact deploying Redpanda in Kubernetes.
  # Make use of the above sections to modify these values instead (see comments below).
  # admin: "127.0.0.1:9644"                                        # Address and port of admin server
  # admin_api_tls: validate_many                                   # TLS configuration for admin HTTP server
  # advertised_kafka_api: None                                     # Address of Kafka API published to the clients
  # advertised_pandaproxy_api: None                                # Rest API address and port to publish to client
  # advertised_rpc_api: None                                       # Address of RPC endpoint published to other cluster members
  # default_topic_partitions: 1                                    # Default number of partitions per topic
  # default_topic_replications: 3                                  # Default replication factor for new topics
  # enable_admin_api: true                                         # Enable the admin API
  # enable_sasl: false                                             # Enable SASL authentication for Kafka connections
  # kafka_api: "127.0.0.1:9092"                                    # Address and port of an interface to listen for Kafka API requests
  # kafka_api_tls: None                                            # TLS configuration for Kafka API endpoint
  # pandaproxy_api: "0.0.0.0:8082"                                 # Rest API listen address and port
  # pandaproxy_api_tls: validate_many                              # TLS configuration for Pandaproxy api
  # rpc_server: "127.0.0.1:33145"                                  # IP address and port for RPC server
  # rpc_server_tls: validate                                       # TLS configuration for RPC server
  # seed_servers: None                                             # List of the seed servers used to join current cluster; If the seed_server list is empty the node will be a cluster root and it will form a new cluster
  # superusers: None                                               # List of superuser usernames
