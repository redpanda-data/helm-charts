---
# Source: redpanda/templates/poddisruptionbudget.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: redpanda
  namespace: default
  labels:
    app.kubernetes.io/component: redpanda
    app.kubernetes.io/instance: redpanda
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redpanda
    helm.sh/chart: redpanda-5.7.39
    testlabel: exercise_common_labels_template
spec:
  maxUnavailable: 1
  selector:
    matchLabels: 
      app.kubernetes.io/component: redpanda-statefulset
      app.kubernetes.io/instance: redpanda
      app.kubernetes.io/name: redpanda
      testlabel: exercise_common_labels_template
      redpanda.com/poddisruptionbudget: redpanda
---
# Source: redpanda/charts/console/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
automountServiceAccountToken: true
metadata:
  name: redpanda-console
  labels:
    helm.sh/chart: console-0.7.26
    app.kubernetes.io/name: console
    app.kubernetes.io/instance: redpanda
    app.kubernetes.io/version: "v2.4.6"
    app.kubernetes.io/managed-by: Helm
---
# Source: redpanda/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: redpanda-sts-lifecycle
  namespace: "default"
  labels:
    app.kubernetes.io/component: redpanda
    app.kubernetes.io/instance: redpanda
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redpanda
    helm.sh/chart: redpanda-5.7.39
    testlabel: exercise_common_labels_template
type: Opaque
stringData:
  common.sh: |-
    #!/usr/bin/env bash

    # the SERVICE_NAME comes from the metadata.name of the pod, essentially the POD_NAME
    CURL_URL="http://${SERVICE_NAME}.redpanda.default.svc.cluster.local:9644"

    # commands used throughout
    CURL_NODE_ID_CMD="curl --silent --fail  ${CURL_URL}/v1/node_config"

    CURL_MAINTENANCE_DELETE_CMD_PREFIX='curl -X DELETE --silent -o /dev/null -w "%{http_code}"'
    CURL_MAINTENANCE_PUT_CMD_PREFIX='curl -X PUT --silent -o /dev/null -w "%{http_code}"'
    CURL_MAINTENANCE_GET_CMD="curl -X GET --silent  ${CURL_URL}/v1/maintenance"

  postStart.sh: |-
    #!/usr/bin/env bash
    # This code should be similar if not exactly the same as that found in the panda-operator, see
    # https://github.com/redpanda-data/redpanda/blob/e51d5b7f2ef76d5160ca01b8c7a8cf07593d29b6/src/go/k8s/pkg/resources/secret.go

    # path below should match the path defined on the statefulset
    source /var/lifecycle/common.sh

    postStartHook () {
      set -x

      touch /tmp/postStartHookStarted

      until NODE_ID=$(${CURL_NODE_ID_CMD} | grep -o '\"node_id\":[^,}]*' | grep -o '[^: ]*$'); do
          sleep 0.5
      done

      echo "Clearing maintenance mode on node ${NODE_ID}"
      CURL_MAINTENANCE_DELETE_CMD="${CURL_MAINTENANCE_DELETE_CMD_PREFIX}  ${CURL_URL}/v1/brokers/${NODE_ID}/maintenance"
      # a 400 here would mean not in maintenance mode
      until [ "${status:-}" = '"200"' ] || [ "${status:-}" = '"400"' ]; do
          status=$(${CURL_MAINTENANCE_DELETE_CMD})
          sleep 0.5
      done

      touch /tmp/postStartHookFinished
    }

    postStartHook
    true

  preStop.sh: |-
    #!/usr/bin/env bash
    # This code should be similar if not exactly the same as that found in the panda-operator, see
    # https://github.com/redpanda-data/redpanda/blob/e51d5b7f2ef76d5160ca01b8c7a8cf07593d29b6/src/go/k8s/pkg/resources/secret.go

    touch /tmp/preStopHookStarted

    # path below should match the path defined on the statefulset
    source /var/lifecycle/common.sh

    set -x

    preStopHook () {
      until NODE_ID=$(${CURL_NODE_ID_CMD} | grep -o '\"node_id\":[^,}]*' | grep -o '[^: ]*$'); do
          sleep 0.5
      done

      echo "Setting maintenance mode on node ${NODE_ID}"
      CURL_MAINTENANCE_PUT_CMD="${CURL_MAINTENANCE_PUT_CMD_PREFIX}  ${CURL_URL}/v1/brokers/${NODE_ID}/maintenance"
      until [ "${status:-}" = '"200"' ]; do
          status=$(${CURL_MAINTENANCE_PUT_CMD})
          sleep 0.5
      done

      until [ "${finished:-}" = "true" ] || [ "${draining:-}" = "false" ]; do
          res=$(${CURL_MAINTENANCE_GET_CMD})
          finished=$(echo $res | grep -o '\"finished\":[^,}]*' | grep -o '[^: ]*$')
          draining=$(echo $res | grep -o '\"draining\":[^,}]*' | grep -o '[^: ]*$')
          sleep 0.5
      done

      touch /tmp/preStopHookFinished
    }
    touch /tmp/preStopHookFinished
    echo "Not enough replicas or in recovery mode, cannot put a broker into maintenance mode."
    true
---
# Source: redpanda/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: redpanda-config-watcher
  namespace: "default"
  labels:
    app.kubernetes.io/component: redpanda
    app.kubernetes.io/instance: redpanda
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redpanda
    helm.sh/chart: redpanda-5.7.39
    testlabel: exercise_common_labels_template
type: Opaque
stringData:
  sasl-user.sh: |-
    #!/usr/bin/env bash

    trap 'error_handler $? $LINENO' ERR

    error_handler() {
      echo "Error: ($1) occurred at line $2"
    }

    set -e

    echo "Waiting for cluster to be ready"
    rpk cluster health --watch --exit-when-healthy
    echo "Nothing to do. Sleeping..."
    sleep infinity
---
# Source: redpanda/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: redpanda-configurator
  namespace: "default"
  labels:
    app.kubernetes.io/component: redpanda
    app.kubernetes.io/instance: redpanda
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redpanda
    helm.sh/chart: redpanda-5.7.39
    testlabel: exercise_common_labels_template
type: Opaque
stringData:
  configurator.sh: |-
    set -xe
    SERVICE_NAME=$1
    KUBERNETES_NODE_NAME=$2
    POD_ORDINAL=${SERVICE_NAME##*-}
    BROKER_INDEX=`expr $POD_ORDINAL + 1`

    CONFIG=/etc/redpanda/redpanda.yaml

    # Setup config files
    cp /tmp/base-config/redpanda.yaml "${CONFIG}"
    cp /tmp/base-config/bootstrap.yaml /etc/redpanda/.bootstrap.yaml

    LISTENER="{\"address\":\"${SERVICE_NAME}.redpanda.default.svc.cluster.local.\",\"name\":\"internal\",\"port\":9093}"
    rpk redpanda config --config "$CONFIG" set redpanda.advertised_kafka_api[0] "$LISTENER"

    ADVERTISED_KAFKA_ADDRESSES=()

    PREFIX_TEMPLATE=""
    ADVERTISED_KAFKA_ADDRESSES+=("{\"address\":\"${SERVICE_NAME}\",\"name\":\"default\",\"port\":31092}")

    rpk redpanda config --config "$CONFIG" set redpanda.advertised_kafka_api[1] "${ADVERTISED_KAFKA_ADDRESSES[$POD_ORDINAL]}"

    LISTENER="{\"address\":\"${SERVICE_NAME}.redpanda.default.svc.cluster.local.\",\"name\":\"internal\",\"port\":8082}"
    rpk redpanda config --config "$CONFIG" set pandaproxy.advertised_pandaproxy_api[0] "$LISTENER"

    ADVERTISED_HTTP_ADDRESSES=()

    PREFIX_TEMPLATE=""
    ADVERTISED_HTTP_ADDRESSES+=("{\"address\":\"${SERVICE_NAME}\",\"name\":\"default\",\"port\":30082}")

    rpk redpanda config --config "$CONFIG" set pandaproxy.advertised_pandaproxy_api[1] "${ADVERTISED_HTTP_ADDRESSES[$POD_ORDINAL]}"
---
# Source: redpanda/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: redpanda
  namespace: "default"
  labels:
    app.kubernetes.io/component: redpanda
    app.kubernetes.io/instance: redpanda
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redpanda
    helm.sh/chart: redpanda-5.7.39
    testlabel: exercise_common_labels_template
data: 
  
  bootstrap.yaml: |
    kafka_enable_authorization: false
    enable_sasl: false
    enable_rack_awareness: false
          
    default_topic_replications: 1
    retention_local_target_ms_default: 21600000
    
    compacted_log_segment_size: 67108864
    group_topic_partitions: 16
    kafka_batch_max_bytes: 1048576
    kafka_connection_rate_limit: 1000
    log_segment_size: 134217728
    log_segment_size_max: 268435456
    log_segment_size_min: 16777216
    max_compacted_log_segment_size: 536870912
    topic_partitions_per_shard: 1000
    storage_min_free_bytes: 161061273
  
    audit_enabled: false
  
  redpanda.yaml: |
    config_file: /etc/redpanda/redpanda.yaml
    redpanda:
      empty_seed_starts_cluster: false
      kafka_enable_authorization: false
      enable_sasl: false
      default_topic_replications: 3
      retention_local_target_ms_default: 21600000
      compacted_log_segment_size: 67108864
      group_topic_partitions: 16
      kafka_batch_max_bytes: 1048576
      kafka_connection_rate_limit: 1000
      log_segment_size: 134217728
      log_segment_size_max: 268435456
      log_segment_size_min: 16777216
      max_compacted_log_segment_size: 536870912
      topic_partitions_per_shard: 1000
      storage_min_free_bytes: 161061273
        
      crash_loop_limit: "5"
      audit_enabled: false
  
  
      admin:
        - name: internal
          address: 0.0.0.0
          port: 9644
        - name: default
          address: 0.0.0.0
          port: 9645
      admin_api_tls:
      kafka_api:
        - name: internal
          address: 0.0.0.0
          port: 9093
        - name: default
          address: 0.0.0.0
          port: 9094
      kafka_api_tls:
      rpc_server:
        address: 0.0.0.0
        port: 33145
      seed_servers: 
        - host:
            address: redpanda-0.redpanda.default.svc.cluster.local.
            port: 33145
  
    schema_registry_client:
      brokers:
      - address: redpanda-0.redpanda.default.svc.cluster.local.
        port: 9093
    schema_registry:
      schema_registry_api:
        - name: internal
          address: 0.0.0.0
          port: 8081
        - name: default
          address: 0.0.0.0
          port: 8084
      schema_registry_api_tls:
  
    pandaproxy_client:
      brokers:
      - address: redpanda-0.redpanda.default.svc.cluster.local.
        port: 9093
    pandaproxy:
      pandaproxy_api:
        - name: internal
          address: 0.0.0.0
          port: 8082
        - name: default
          address: 0.0.0.0
          port: 8083
      pandaproxy_api_tls:
  
    
    rpk:
      # redpanda server configuration
      overprovisioned: false
      enable_memory_locking: false
      additional_start_flags:
        - --default-log-level=info
        - --memory=2048M
        - --reserve-memory=205M
        - --smp=1
      # rpk tune entries
      tune_aio_events: true
    
      # kafka connection configuration
      kafka_api:
        brokers: 
          - redpanda-0.redpanda.default.svc.cluster.local.:9093
        tls:
      admin_api:
        addresses: 
          - redpanda-0.redpanda.default.svc.cluster.local.:9644
        tls:
---
# Source: redpanda/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: redpanda-rpk
  namespace: "default"
  labels:
    app.kubernetes.io/component: redpanda
    app.kubernetes.io/instance: redpanda
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redpanda
    helm.sh/chart: redpanda-5.7.39
    testlabel: exercise_common_labels_template
data:
  profile: | 
    name: default
    kafka_api:
      brokers: 
          - redpanda-0:31092
      tls:
    admin_api:
      addresses: 
          - redpanda-0:31644
      tls:
---
# Source: redpanda/templates/console/configmap-and-deployment.yaml
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: redpanda-console
  labels:
    helm.sh/chart: console-0.7.26
    app.kubernetes.io/name: console
    app.kubernetes.io/instance: redpanda
    app.kubernetes.io/version: "v2.4.6"
    app.kubernetes.io/managed-by: Helm
    
data:
  config.yaml: |
    # from .Values.console.config
    connect: {}
    kafka:
      brokers:
      - redpanda-0.redpanda.default.svc.cluster.local.:9093
      sasl:
        enabled: false
      schemaRegistry:
        enabled: true
        tls:
          enabled: false
        urls:
        - http://redpanda-0.redpanda.default.svc.cluster.local.:8081
      tls:
        enabled: false
---
# Source: redpanda/charts/console/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: redpanda-console
  labels:
    helm.sh/chart: console-0.7.26
    app.kubernetes.io/name: console
    app.kubernetes.io/instance: redpanda
    app.kubernetes.io/version: "v2.4.6"
    app.kubernetes.io/managed-by: Helm
    
spec:
  type: ClusterIP
  ports:
    - port: 8080
      targetPort: 
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: console
    app.kubernetes.io/instance: redpanda
---
# Source: redpanda/templates/service.internal.yaml
# This service is only used to create the DNS enteries for each pod in
# the stateful set and allow the serviceMonitor to target the pods.
# This service should not be used by any client application
apiVersion: v1
kind: Service
metadata:
  name: redpanda
  namespace: "default"
  labels:
    monitoring.redpanda.com/enabled: "false"
    app.kubernetes.io/component: redpanda
    app.kubernetes.io/instance: redpanda
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redpanda
    helm.sh/chart: redpanda-5.7.39
    testlabel: exercise_common_labels_template
spec:
  type: ClusterIP
  publishNotReadyAddresses: true
  clusterIP: None
  selector: 
    app.kubernetes.io/component: redpanda-statefulset
    app.kubernetes.io/instance: redpanda
    app.kubernetes.io/name: redpanda
    testlabel: exercise_common_labels_template
  ports:
    - name: admin
      protocol: TCP
      port: 9644
      targetPort: 9644
    - name: http
      protocol: TCP
      port: 8082
      targetPort: 8082
    - name: kafka
      protocol: TCP
      port: 9093
      targetPort: 9093
    - name: rpc
      protocol: TCP
      port: 33145
      targetPort: 33145
    - name: schemaregistry
      protocol: TCP
      port: 8081
      targetPort: 8081
---
# Source: redpanda/templates/service.nodeport.yaml
apiVersion: v1
kind: Service
metadata:
  annotations: {}
  creationTimestamp: null
  labels:
    app.kubernetes.io/component: redpanda
    app.kubernetes.io/instance: redpanda
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redpanda
    helm.sh/chart: redpanda-5.7.39
    testlabel: exercise_common_labels_template
  name: redpanda-external
  namespace: default
spec:
  externalTrafficPolicy: Local
  ports:
  - name: admin-default
    nodePort: 31644
    port: 9645
    protocol: TCP
    targetPort: 0
  - name: kafka-default
    nodePort: 31092
    port: 9094
    protocol: TCP
    targetPort: 0
  - name: http-default
    nodePort: 30082
    port: 8083
    protocol: TCP
    targetPort: 0
  - name: schema-default
    nodePort: 30081
    port: 8084
    protocol: TCP
    targetPort: 0
  publishNotReadyAddresses: true
  selector:
    app.kubernetes.io/component: redpanda-statefulset
    app.kubernetes.io/instance: redpanda
    app.kubernetes.io/name: redpanda
    testlabel: exercise_common_labels_template
  sessionAffinity: None
  type: NodePort
status:
  loadBalancer: {}
---
# Source: redpanda/templates/console/configmap-and-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redpanda-console
  labels:
    helm.sh/chart: console-0.7.26
    app.kubernetes.io/name: console
    app.kubernetes.io/instance: redpanda
    app.kubernetes.io/version: "v2.4.6"
    app.kubernetes.io/managed-by: Helm
    
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: console
      app.kubernetes.io/instance: redpanda
  template:
    metadata:
      annotations:
        checksum/config: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
        checksum-redpanda-chart/config: 4636a43ba7f19845dafb63c50d046582b8605eb99db7a02f58148c904ada0e54
      labels:
        app.kubernetes.io/name: console
        app.kubernetes.io/instance: redpanda
    spec:
      serviceAccountName: redpanda-console
      automountServiceAccountToken: true
      securityContext:
        fsGroup: 99
        runAsUser: 99
      volumes:
        - name: configs
          configMap:
            name: redpanda-console
      containers:
        - name: console
          args:
            - "--config.filepath=/etc/console/configs/config.yaml"
          securityContext:
            runAsNonRoot: true
          image: docker.redpanda.com/redpandadata/console:v2.4.6
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
          volumeMounts:
            - name: configs
              mountPath: /etc/console/configs
              readOnly: true
          livenessProbe:
            initialDelaySeconds: 0
            periodSeconds: 10
            timeoutSeconds: 1
            successThreshold: 1
            failureThreshold: 3
            httpGet:
              path: /admin/health
              port: http
          readinessProbe:
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 1
            successThreshold: 1
            failureThreshold: 3
            httpGet:
              path: /admin/health
              port: http
          resources:
            {}
          env:
            - name: REDPANDA_ADMINAPI_ENABLED
              value: "true"
            - name: REDPANDA_ADMINAPI_URLS
              value: http://redpanda.default.svc.cluster.local.:9644
      priorityClassName:
---
# Source: redpanda/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: redpanda
  labels:
    app.kubernetes.io/component: redpanda
    app.kubernetes.io/instance: redpanda
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redpanda
    helm.sh/chart: redpanda-5.7.39
    testlabel: exercise_common_labels_template
spec:
  selector:
    matchLabels: 
      app.kubernetes.io/component: redpanda-statefulset
      app.kubernetes.io/instance: redpanda
      app.kubernetes.io/name: redpanda
      testlabel: exercise_common_labels_template
  serviceName: redpanda
  replicas: 1
  updateStrategy:
    type: RollingUpdate
  podManagementPolicy: "Parallel"
  template:
    metadata:
      labels: 
        app.kubernetes.io/component: redpanda-statefulset
        app.kubernetes.io/instance: redpanda
        app.kubernetes.io/name: redpanda
        redpanda.com/poddisruptionbudget: redpanda
        testlabel: exercise_common_labels_template
      annotations:
        config.redpanda.com/checksum: c25c6cf5baf4214ae98067f0a26d62a97fd24d0d059b10724cbaadd0e8097d82
    spec:
      terminationGracePeriodSeconds: 90
      securityContext: 
        fsGroup: 101
        fsGroupChangePolicy: OnRootMismatch
      serviceAccountName: default
      initContainers:
        - name: tuning
          image: docker.redpanda.com/redpandadata/redpanda:v23.3.11
          command:
            - /bin/bash
            - -c
            - rpk redpanda tune all
          securityContext:
            capabilities:
              add: ["SYS_RESOURCE"]
            privileged: true
            runAsUser: 0
            runAsGroup: 0
          volumeMounts: 
            
            - name: redpanda
              mountPath: /etc/redpanda
        - name: redpanda-configurator
          image: docker.redpanda.com/redpandadata/redpanda:v23.3.11
          command:
            - /bin/bash
            - -c
            - 'trap "exit 0" TERM; exec $CONFIGURATOR_SCRIPT "${SERVICE_NAME}" "${KUBERNETES_NODE_NAME}" & wait $!'
          env:
            - name: CONFIGURATOR_SCRIPT
              value: /etc/secrets/configurator/scripts/configurator.sh
            - name: SERVICE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: KUBERNETES_NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            - name: HOST_IP_ADDRESS
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: status.hostIP
          securityContext: 
            runAsUser: 101
            runAsGroup: 101
          volumeMounts: 
            
            - name: config
              mountPath: /etc/redpanda
            - name: redpanda
              mountPath: /tmp/base-config
            - name: redpanda-configurator
              mountPath: /etc/secrets/configurator/scripts/
      containers:
        - name: redpanda
          image: docker.redpanda.com/redpandadata/redpanda:v23.3.11
          env:
            - name: SERVICE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: HOST_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.hostIP
          # finish the lifecycle scripts with "true" to prevent them from terminating the pod prematurely
          lifecycle:
            postStart:
              exec:
                command:
                  - /bin/bash
                  - -c
                  - |
                    timeout -v 45 bash -x /var/lifecycle/postStart.sh
                    true
            preStop:
              exec:
                command:
                  - /bin/bash
                  - -c
                  - |
                    timeout -v 45 bash -x /var/lifecycle/preStop.sh
                    true # do not fail and cause the pod to terminate
          # the startupProbe checks to see that the admin api is listening and that the broker has a node_id assigned. This
          # check is only used to delay the start of the liveness and readiness probes until it passes.
          startupProbe:
            exec:
              command:
                - /bin/sh
                - -c
                - |
                  set -e
                  RESULT=$(curl --silent --fail -k -m 5  "http://${SERVICE_NAME}.redpanda.default.svc.cluster.local.:9644/v1/status/ready")
                  echo $RESULT
                  echo $RESULT | grep ready
            initialDelaySeconds: 1
            failureThreshold: 120
            periodSeconds: 10
          # the livenessProbe just checks to see that the admin api is listening and returning 200s.
          livenessProbe:
            exec:
              command:
                - /bin/sh
                - -c
                - curl --silent --fail -k -m 5  "http://${SERVICE_NAME}.redpanda.default.svc.cluster.local.:9644/v1/status/ready"
            initialDelaySeconds: 10
            failureThreshold: 3
            periodSeconds: 10
          # the readiness probe just checks that the cluster is healthy according to rpk cluster health.
          # It's ok that this cluster-wide check affects all the pods as it's only used for the
          # PodDisruptionBudget and we don't want to roll any pods if the Redpanda cluster isn't healthy.
          # https://kubernetes.io/docs/concepts/workloads/pods/disruptions/#pod-disruption-budgets
          # All services set `publishNotReadyAddresses:true` to prevent this from affecting cluster access
          readinessProbe:
            exec:
              command:
                - /bin/sh
                - -c
                - |
                  set -x
                  RESULT=$(rpk cluster health)
                  echo $RESULT
                  echo $RESULT | grep 'Healthy:.*true'
            initialDelaySeconds: 1
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
          command:
            - rpk
            - redpanda
            - start
            - "--advertise-rpc-addr=$(SERVICE_NAME).redpanda.default.svc.cluster.local.:33145"
          ports:
            - name: admin
              containerPort: 9644
            - name: admin-default
              containerPort: 9645
            - name: http
              containerPort: 8082
            - name: http-default
              containerPort: 8083
            - name: kafka
              containerPort: 9093
            - name: kafka-default
              containerPort: 9094
            - name: rpc
              containerPort: 33145
            - name: schemaregistry
              containerPort: 8081
            - name: schema-default
              containerPort: 8084
          securityContext: 
            runAsUser: 101
            runAsGroup: 101
          volumeMounts: 
            
            - name: config
              mountPath: /etc/redpanda
            - name: redpanda
              mountPath: /tmp/base-config
            - name: lifecycle-scripts
              mountPath: /var/lifecycle
            - name: datadir
              mountPath: /var/lib/redpanda/data
          resources:
            limits:
              cpu: 1
              memory: 2.5Gi
        - name: config-watcher
          image: docker.redpanda.com/redpandadata/redpanda:v23.3.11
          command:
            - /bin/sh
          args:
            - -c
            - 'trap "exit 0" TERM; exec /etc/secrets/config-watcher/scripts/sasl-user.sh & wait $!'
          volumeMounts: 
            
            - name: config
              mountPath: /etc/redpanda
            - name: redpanda-config-watcher
              mountPath: /etc/secrets/config-watcher/scripts
      volumes: 
        
        - name: lifecycle-scripts
          secret:
            secretName: redpanda-sts-lifecycle
            defaultMode: 0o775
        - name: datadir
          persistentVolumeClaim:
            claimName: datadir
        - name: redpanda
          configMap:
            name: redpanda
        - name: config
          emptyDir: {}
        - name: redpanda-configurator
          secret:
            secretName: redpanda-configurator
            defaultMode: 0o775
        - name: redpanda-config-watcher
          secret:
            secretName: redpanda-config-watcher
            defaultMode: 0o775
        - name: redpanda-fs-validator
          secret:
            secretName: redpanda-fs-validator
            defaultMode: 0o775
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels: 
              app.kubernetes.io/component: redpanda-statefulset
              app.kubernetes.io/instance: redpanda
              app.kubernetes.io/name: redpanda
              testlabel: exercise_common_labels_template
      nodeSelector:
        {}
      affinity:
        
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - topologyKey: kubernetes.io/hostname
            labelSelector:
              matchLabels: 
                app.kubernetes.io/component: redpanda-statefulset
                app.kubernetes.io/instance: redpanda
                app.kubernetes.io/name: redpanda
                testlabel: exercise_common_labels_template
      tolerations:
        []
  volumeClaimTemplates:
    - metadata:
        name: datadir
        labels:
          app.kubernetes.io/name: redpanda
          app.kubernetes.io/instance: "redpanda"
          app.kubernetes.io/component: redpanda
          testlabel: exercise_common_labels_template
      spec:
        accessModes: ["ReadWriteOnce"]
        resources:
          requests:
            storage: "3Gi"
---
# Source: redpanda/templates/console/configmap-and-deployment.yaml
# before license changes, this was not printing a secret, so we gather in which case to print
# for now only if we have a license do we print, however, this may be an issue for some
# since if we do include a license we MUST also print all secret items.
---
# Source: redpanda/charts/console/templates/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "redpanda-console-test-connection"
  labels:
    helm.sh/chart: console-0.7.26
    app.kubernetes.io/name: console
    app.kubernetes.io/instance: redpanda
    app.kubernetes.io/version: "v2.4.6"
    app.kubernetes.io/managed-by: Helm
    
  annotations:
    "helm.sh/hook": test
spec:
  containers:
    - name: wget
      image: busybox
      command: ['wget']
      args: ['redpanda-console:8080']
  restartPolicy: Never
  priorityClassName:
---
# Source: redpanda/templates/post-install-upgrade-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: redpanda-configuration
  namespace: "default"
  labels:
    app.kubernetes.io/component: redpanda
    app.kubernetes.io/instance: redpanda
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redpanda
    helm.sh/chart: redpanda-5.7.39
    testlabel: exercise_common_labels_template
  annotations:
    # This is what defines this resource as a hook. Without this line, the
    # job is considered part of the release.
    "helm.sh/hook": post-install,post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation
    "helm.sh/hook-weight": "-5"
spec:
  template:
    metadata:
      generateName: "redpanda-post-"
      labels:
        app.kubernetes.io/name: redpanda
        app.kubernetes.io/instance: "redpanda"
        app.kubernetes.io/component: redpanda-post-install
        testlabel: exercise_common_labels_template
    spec:
      affinity:
        {}
      restartPolicy: Never
      securityContext: 
        fsGroup: 101
        fsGroupChangePolicy: OnRootMismatch
      containers:
      - name: redpanda-post-install
        image: docker.redpanda.com/redpandadata/redpanda:v23.3.11
        
        env: []
        command: ["bash","-c"]
        args:
          - |
            set -e
            if [[ -n "$REDPANDA_LICENSE" ]] then
              rpk cluster license set "$REDPANDA_LICENSE"
            fi

            

            
            rpk cluster config export -f /tmp/cfg.yml

            
            for KEY in "${!RPK_@}"; do
              config="${KEY#*RPK_}"
              rpk redpanda config set --config /tmp/cfg.yml "${config,,}" "${!KEY}"
            done

            
            rpk cluster config import -f /tmp/cfg.yml
        securityContext:
          runAsGroup: 101
          runAsUser: 101
        volumeMounts:
          - name: config
            mountPath: /etc/redpanda
      volumes: 
        - name: config
          configMap:
            name: redpanda
      serviceAccountName: default
---
# Source: redpanda/templates/post-upgrade.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: redpanda-post-upgrade
  namespace: "default"
  labels:
    app.kubernetes.io/component: redpanda
    app.kubernetes.io/instance: redpanda
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redpanda
    helm.sh/chart: redpanda-5.7.39
    testlabel: exercise_common_labels_template
  annotations:
    "helm.sh/hook": post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation
    "helm.sh/hook-weight": "-10"
spec:
  template:
    metadata:
      name: "redpanda"
      labels:
        app.kubernetes.io/name: redpanda
        app.kubernetes.io/instance: "redpanda"
        app.kubernetes.io/component: redpanda-post-upgrade
        testlabel: exercise_common_labels_template
    spec:
      affinity:
        {}
      restartPolicy: Never
      securityContext: 
        fsGroup: 101
        fsGroupChangePolicy: OnRootMismatch
      serviceAccountName: default
      containers:
      - name: redpanda-post-upgrade
        image: docker.redpanda.com/redpandadata/redpanda:v23.3.11
        command: ["/bin/bash", "-c"]
        args:
          - |
            set -e
    
            rpk cluster config set default_topic_replications 1
            rpk cluster config set retention_local_target_ms_default 21600000
            rpk cluster config set storage_min_free_bytes 161061273
            if [ -d "/etc/secrets/users/" ]; then
                IFS=":" read -r USER_NAME PASSWORD MECHANISM < <(grep "" $(find /etc/secrets/users/* -print))
                curl -svm3 --fail --retry "120" --retry-max-time "120" --retry-all-errors --ssl-reqd \
                --cacert /etc/tls/certs/default/ca.crt \
                -X PUT -u ${USER_NAME}:${PASSWORD} \
                http://redpanda.default.svc.cluster.local.:9644/v1/debug/restart_service?service=schema-registry || true
            fi
        securityContext:
          runAsGroup: 101
          runAsUser: 101
        volumeMounts:
          - name: config
            mountPath: /etc/redpanda
      volumes: 
        - name: config
          configMap:
            name: redpanda
