# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# This file contains values for variables referenced from yaml files in the templates directory.
#
# For further information on Helm templating see the documentation at:
#  https://helm.sh/docs/chart_template_guide/values_files/

# Common parameters
#
# Override the app.kubernetes.io/name label
nameOverride: ""
# Objects are named based on the name of the helm release. fullnameOverride,
# overrides this behavior naming resources as specified.
fullnameOverride: ""
# Cluster's default DNS domain.
# You should overwrite it if you're using a different one,
# otherwise Redpanda node discovery and TLS certificates won't work.
clusterDomain: cluster.local
# Additional labels added to all Kubernetes objects
commonLabels: {}

# Redpanda parameters
#
image:
  repository: vectorized/redpanda
  # Redpanda version. This determines the installed version (not Chart.appVersion)
  tag: v22.1.6
  # The imagePullPolicy will default to Always when the tag is 'latest'
  pullPolicy: IfNotPresent
# Your license key (optional)
license_key: ""
#
# Authentication
auth:
  #
  # SASL configuration
  sasl:
    enabled: false
    # user list
    # TODO create user at startup
    users:
      - name: admin
        # Password for the user. This will be used to generate a secret
        # password: password
        # If password isn't given, then the secretName must point to an already existing secret
        # secretName: adminPassword
  #
  # TLS configuration
  tls:
    # Enable global TLS, which turns on TLS by default for all listeners
    # Each listener must include a certificate name in its TLS section
    # Any certificates in auth.tls.certs will still be loaded if enabled is false
    # This is because listeners may enable TLS individually (see listeners.<listener name>.tls.enabled)
    enabled: false
    # list all certificates below, then reference a certificate's name in each listener (see listeners.<listener name>.tls.cert)
    certs:
      # This is the certificate name that is used to associate the certificate with a listener
      # See listeners.<listener group>.tls.cert for more information
      kafka:
        # issuerRef:
        #   name: redpanda-cert1-selfsigned-issuer
        # The caEnabled flag determines whether the ca.crt file is included in the TLS mount path on each Redpanda pod
        caEnabled: true
        # duration: 43800h
      rpc:
        caEnabled: true
      admin:
        caEnabled: true
      proxy:
        caEnabled: true
      schema:
        caEnabled: true
#
# External access configuration
external:
  # Default external access value for all listeners except RPC
  # External config doesn't apply to RPC listeners as they are never externally accessible
  # These values can be overridden by each listener if needed
  enabled: true
  # Default external access type (options are NodePort and LoadBalancer)
  # TODO include IP range for load balancer that support it: https://github.com/redpanda-data/helm-charts/issues/106
  type: NodePort
  # This domain name will be appended to the pod name and added to the `advertised_kafka_api`
  # node configuration flag.
  domain: local
  # annotations:
    # For example:
    # cloud.google.com/load-balancer-type: "Internal"
    # service.beta.kubernetes.io/aws-load-balancer-type: nlb

# Logging
logging:
  # Log level
  # Valid values (from least to most logging) are warn, info, debug, trace
  logLevel: info
  #
  # Send usage stats back to Redpanda
  # See https://docs.redpanda.com/docs/cluster-administration/monitoring/#stats-reporting
  usageStats:
    # rpk.enable_usage_stats
    enabled: true
    # Your organization name (optional)
    # organization: your-org
    # Your cluster ID (optional)
    # clusterId: your-helm-cluster
#
resources:
  # Both Redpanda and Kubernetes have multiple ways to allocate resources.
  # There are also several associated parameters that impact how these resources are used by
  # Kubernetes, the Redpanda app, and the subsystem Redpanda is built on (Seastar).
  # This section attempts to simplify allocating resources by providing a single location
  # where resources are defined.
  # Helm sets these resource values within the following templates:
  # - statefulset.yaml
  # - configmap.yaml
  #
  # The default values below are what should work for a development environment.
  # Production-level values and other considerations are provided in comments
  # if those values are different from the default.
  #
  cpu:
    # Redpanda makes use of a thread per core model described here:
    # https://redpanda.com/blog/tpc-buffers
    # For this reason, Redpanda should only be given full cores (cores parameter below).
    #
    # NOTE: You can increase cores, but decreasing cores is not currently supported:
    # https://github.com/redpanda-data/redpanda/issues/350
    #
    # Equivalent to: --smp, resources.requests.cpu, and resources.limits.cpu
    # For production: 4 or greater
    cores: 1
    #
    # Overprovisioned means Redpanda won't assume it has all of the provisioned CPU.
    # This should be true unless the container has CPU affinity (eg. min and max above are equal).
    # Equivalent to: --idle-poll-time-us 0 --thread-affinity 0 --poll-aio 0
    # overprovisioned: false
  #
  memory:
    # Enables memory locking.
    # For production: true
    # enable_memory_locking: false
    #
    # It is recommended to have at least 2Gi of memory per core for the Redpanda binary.
    # This memory is taken from the total memory given to each container.
    # We allocate 80% of the container's memory to Redpanda, leaving the rest for
    # the Seastar subsystem (reserveMemory) and other container processes.
    # So at least 2.5Gi per core is recommended in order to ensure Redpanda has a full 2Gi.
    #
    # These values affect --memory and --reserve-memory flags passed to Redpanda and the memory
    # requests/limits in the StatefulSet.
    # Valid suffixes: k M G Ki Mi Gi
    # Only support a single decimal (eg. 2.5Gi rather than 2.55Gi)
    #
    container:
      # Minimum memory count for each Redpanda broker
      # If omitted, the min value will equal the max value (requested resources defaults to limits)
      # Equivalent to: resources.requests.memory
      # For production: 10Gi or greater
      # min: 2.5Gi
      #
      # Minimum memory count for each Redpanda broker
      # Equivalent to: resources.limits.memory
      # For production: 10Gi or greater
      max: 2.5Gi
    #
    # redpanda:
      # This optional redpanda section allows specifying the memory size for both the Redpanda
      # process and the underlying reserved memory (used by Seastar).
      # This section is omitted by default, and memory sizes are calculated automatically
      # based on container memory.
      # Uncommenting this section and setting memory and reserveMemory values will disable
      # automatic calculation.
      #
      # If you are setting the following values manually, keep in mind the following guidelines (getting
      # this wrong will potentially lead to performance issues, instability, loss of data, etc.):
      # The amount of memory to allocate to a container is determined by the sum of three values:
      # 1. Redpanda (at least 2Gi per core, ~80% of the container's total memory)
      # 2. Seastar subsystem (200Mi * 0.2% of the container's total memory, 200Mi < x < 1Gi)
      # 3. other container processes (whatever small amount remains)
      #
      # Memory for the Redpanda process.
      # This must be lower the container's memory (resources.memory.container.min if provided, otherwise
      # resources.memory.container.max).
      # Equivalent to: --memory
      # For production: 8Gi or greater
      # memory: 2Gi
      #
      # Memory reserved for the Seastar subsystem.
      # Any value above 1Gi will provide diminishing performance benefits.
      # Equivalent to: --reserve-memory
      # For production: 1Gi
      # reserveMemory: 200Mi
#
# Persistence
storage:
  # Absolute path on host to store Redpanda's data.
  # If not specified, then `emptyDir` will be used instead.
  # If specified, but `persistentVolume.enabled` is `true`, then has no effect.
  hostPath: ""
  # If `enabled` is `true` then a PersistentVolumeClaim will be created and
  # used to store Redpanda's data, otherwise `hostPath` is used.
  persistentVolume:
    enabled: true
    size: 3Gi
    # Define storageClass to choose a specific StorageClass from your kubernetes installation.
    # Set to "-", to disables dynamic provisioning.
    # Leave empty (default) to allow kubernetes to set the storage class to
    # the cluster default. To get information about the default storage class, run:
    # ```
    # kubectl describe storageclass $(kubectl get storageclass -o=jsonpath='{.items[?(@.metadata.annotations.storageclass\.kubernetes\.io/is-default-class=="true")].metadata.name}')
    # ```
    storageClass: ""
    # Additional labels to apply to the created PersistentVolumeClaims.
    labels: {}
    # Additional annotations to apply to the created PersistentVolumeClaims.
    annotations: {}

statefulset:
  # Number of Redpanda brokers (recommend setting this to the number of nodes in the cluster)
  replicas: 3
  updateStrategy:
    type: RollingUpdate
  podManagementPolicy: OrderedReady
  budget:
    maxUnavailable: 1
  # Additional annotations to apply to the Pods of this StatefulSet.
  annotations: {}
  #
  # A note regarding statefulset resources:
  # Resources are set through the top-level resources section above.
  # It is recommended to set resources values in that section rather than here, as this will guarantee
  # memory is allocated across containers, Redpanda, and the Seastar subsystem correctly.
  # This automatic memory allocation is in place because Repanda and the Seastar subsystem require flags
  # at startup that set the amount of memory available to each process.
  # Kubernetes (mainly statefulset), Redpanda, and Seastar memory values are tightly coupled.
  # Adding a resource section here will be ignored.
  #
  # Inter-Pod Affinity rules for scheduling Pods of this StatefulSet.
  # https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  podAffinity: {}
  # Anti-affinity rules for scheduling Pods of this StatefulSet.
  # https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  # You may either toggle options below for default anti-affinity rules,
  # or specify the whole set of anti-affinity rules instead of them.
  podAntiAffinity:
    # The topologyKey to be used.
    # Can be used to spread across different nodes, AZs, regions etc.
    topologyKey: kubernetes.io/hostname
    # Type of pod anti-affinity rules: either `soft`, `hard` or empty value (which
    # disables anti-affinity rules). Pod anti-affinity rules either try to place (soft) or require that (hard)
    # pods be assigned to different worker nodes.
    type: soft
    # Weight for `soft` anti-affinity rules.
    # Does not apply for other anti-affinity types.
    weight: 100
  # Node selection constraints for scheduling Pods of this StatefulSet.
  # https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector
  nodeSelector: {}
  # PriorityClassName given to Pods of this StatefulSet
  # https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/#priorityclass
  priorityClassName: ""
  # Taints to be tolerated by Pods of this StatefulSet.
  # https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  tolerations: []
  # https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/
  topologySpreadConstraints:
    maxSkew: 1
    topologyKey: topology.kubernetes.io/zone
    whenUnsatisfiable: ScheduleAnyway
  # When using persistent storage the volume will be mounted as root. In order for redpanda to use the volume
  # we must set the fsGroup to the uid of redpanda, which is 101
  podSecurityContext:
    fsGroup: 101
    # runAsNonRoot: true
    # runAsUser: 1000

# Service account management
serviceAccount:
  # Specifies whether a service account should be created
  create: false
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

tuning: {}
  # This section contains Redpanda tuning parameters.
  # Each parameter below is set to their default values.
  # Remove the curly brackets above if you uncomment any parameters below.
  #
  # Increases the number of allowed asynchronous IO events.
  # tune_aio_events: false
  #
  # Syncs NTP
  # tune_clocksource: false
  #
  # Creates a "ballast" file so that, if a Redpanda node runs out of space,
  # you can delete the ballast file to allow the node to resume operations and then
  # delete a topic or records to reduce the space used by Redpanda.
  # tune_ballast_file: false
  #
  # The path where the ballast file will be created.
  # ballast_file_path: "/var/lib/redpanda/data/ballast"
  #
  # The ballast file size.
  # ballast_file_size: "1GiB"
  #
  # (Optional) The vendor, VM type and storage device type that redpanda will run on, in
  # the format <vendor>:<vm>:<storage>. This hints to rpk which configuration values it
  # should use for the redpanda IO scheduler.
  # Some valid values are "gcp:c2-standard-16:nvme", "aws:i3.xlarge:default"
  # well_known_io: ""
  #
  # The following tuning parameters must be false in container environments and will be ignored:
  #   tune_network
  #   tune_disk_scheduler
  #   tune_disk_nomerges
  #   tune_disk_irq
  #   tune_fstrim
  #   tune_cpu
  #   tune_swappiness
  #   tune_transparent_hugepages
  #   tune_coredump

# This section allows modifying various Redpanda settings not covered in other sections above.
# These values do not pertain to the kubernetes objects created with helm.
# Instead these parameters get passed directly to the Redpanda binary at startup.
# See https://docs.redpanda.com/docs/cluster-administration/configuration/
config:
  cluster:
    # auto_create_topics_enabled: true                             # Allow topic auto creation
    # transaction_coordinator_replication: 1                       # Replication factor for a transaction coordinator topic
    # id_allocator_replication: 1                                  # Replication factor for an ID allocator topic
    # disable_metrics: false                                       # Disable registering metrics
    # enable_coproc: false                                         # Enable coprocessing mode
    # enable_idempotence: false                                    # Enable idempotent producer
    # enable_pid_file: true                                        # Enable pid file; You probably don't want to change this
    # enable_transactions: false                                   # Enable transactions
    # group_max_session_timeout_ms: 300s                           # The maximum allowed session timeout for registered consumers; Longer timeouts give consumers more time to process messages in between heartbeats at the cost of a longer time to detect failures; Default quota tracking window size in milliseconds
    # group_min_session_timeout_ms: Optional                       # The minimum allowed session timeout for registered consumers; Shorter timeouts result in quicker failure detection at the cost of more frequent consumer heartbeating
    # kafka_group_recovery_timeout_ms: 30000ms                     # Kafka group recovery timeout expressed in milliseconds
    # kafka_qdc_enable: false                                      # Enable kafka queue depth control
    # kafka_qdc_max_latency_ms: 80ms                               # Max latency threshold for kafka queue depth control depth tracking
    # log_cleanup_policy: deletion                                 # Default topic cleanup policy
    # log_compaction_interval_ms: 5min                             # How often do we trigger background compaction
    # log_compression_type: producer                               # Default topic compression type
    # log_message_timestamp_type: create_time                      # Default topic messages timestamp type
    # retention_bytes: None                                        # max bytes per partition on disk before triggering a compaction
    # rm_sync_timeout_ms: 2000ms
    # rm_violation_recovery_policy: crash                          # Describes how to recover from an invariant violation happened on the partition level
    # target_quota_byte_rate: 2GB                                  # Target quota byte rate in bytes per second
    # tm_sync_timeout_ms: 2000ms                                   # Time to wait state catch up before rejecting a request
    # tm_violation_recovery_policy: crash                          # Describes how to recover from an invariant violation happened on the transaction coordinator level
    # transactional_id_expiration_ms: 10080min                     # Producer ids are expired once this time has elapsed after the last write with the given producer ID
  tunable:
    # alter_topic_cfg_timeout_ms: 5s                              # Time to wait for entries replication in controller log when executing alter configuration request
    # compacted_log_segment_size: 256MiB                           # How large in bytes should each compacted log segment be (default 256MiB)
    # controller_backend_housekeeping_interval_ms: 1s              # Interval between iterations of controller backend housekeeping loop
    # coproc_max_batch_size: 32kb                                  # Maximum amount of bytes to read from one topic read
    # coproc_max_inflight_bytes: 10MB                              # Maximum amountt of inflight bytes when sending data to wasm engine
    # coproc_max_ingest_bytes: 640kb                               # Maximum amount of data to hold from input logs in memory
    # coproc_offset_flush_interval_ms: 300000ms                    # Interval for which all coprocessor offsets are flushed to disk
    # create_topic_timeout_ms: 2000ms                              # Timeout (ms) to wait for new topic creation
    # default_num_windows: 10                                      # Default number of quota tracking windows
    # default_window_sec: 1000ms                                   # Default quota tracking window size in milliseconds
    # delete_retention_ms: 10080min                                # delete segments older than this (default 1 week)
    # developer_mode: optional                                     # Skips most of the checks performed at startup
    # disable_batch_cache: false                                   # Disable batch cache in log manager
    # fetch_reads_debounce_timeout: 1ms                            # Time to wait for next read in fetch request when requested min bytes wasn't reached
    # fetch_session_eviction_timeout_ms: 60s                       # Minimum time before which unused session will get evicted from sessions; Maximum time after which inactive session will be deleted is two time given configuration valuecache
    # group_initial_rebalance_delay: 300ms                         # Extra delay (ms) added to rebalance phase to wait for new members
    # group_new_member_join_timeout: 30000ms                       # Timeout for new member joins
    # group_topic_partitions: 1                                    # Number of partitions in the internal group membership topic
    # id_allocator_batch_size: 1000                                # ID allocator allocates messages in batches (each batch is a one log record) and then serves requests from memory without touching the log until the batch is exhausted
    # id_allocator_log_capacity: 100                               # Capacity of the id_allocator log in number of messages; Once it reached id_allocator_stm should compact the log
    # join_retry_timeout_ms: 5s                                    # Time between cluster join retries in milliseconds
    # kafka_qdc_idle_depth: 10                                     # Queue depth when idleness is detected in kafka queue depth control
    # kafka_qdc_latency_alpha: 0.002                               # Smoothing parameter for kafka queue depth control latency tracking
    # kafka_qdc_max_depth: 100                                     # Maximum queue depth used in kafka queue depth control
    # kafka_qdc_min_depth: 1                                       # Minimum queue depth used in kafka queue depth control
    # kafka_qdc_window_count: 12                                   # Number of windows used in kafka queue depth control latency tracking
    # kafka_qdc_window_size_ms: 1500ms                             # Window size for kafka queue depth control latency tracking
    # kvstore_flush_interval: 10ms                                 # Key-value store flush interval (ms)
    # kvstore_max_segment_size: 16MB                               # Key-value maximum segment size (bytes)
    # log_segment_size: 1GB                                        # How large in bytes should each log segment be (default 1G)
    # max_compacted_log_segment_size: 5GB                          # Max compacted segment size after consolidation
    # max_kafka_throttle_delay_ms: 60000ms                         # Fail-safe maximum throttle delay on kafka requests
    # metadata_dissemination_interval_ms: 3000ms                   # Interaval for metadata dissemination batching
    # metadata_dissemination_retries: 10                           # Number of attempts of looking up a topic's meta data like shard before failing a request
    # metadata_dissemination_retry_delay_ms: 500ms                 # Delay before retry a topic lookup in a shard or other meta tables
    # quota_manager_gc_sec: 30000ms                                # Quota manager GC frequency in milliseconds
    # raft_learner_recovery_rate: 104857600                          # Raft learner recovery rate in bytes per second
    # raft_heartbeat_disconnect_failures: 3 #After how many failed heartbeats to forcibly close an unresponsive TCP connection. Set to 0 to disable force disconnection.
    # raft_heartbeat_interval_ms: 150 #The interval in ms between raft leader heartbeats.
    # raft_heartbeat_timeout_ms: 3000	#Raft heartbeat RPC timeout.
    # raft_io_timeout_ms: 10000	#Raft I/O timeout.
    # raft_max_concurrent_append_requests_per_follower: 16	#Maximum number of concurrent append entries requests sent by leader to one follower.
    # raft_max_recovery_memory: 33554432	#Maximum memory that can be used for reads in the raft recovery process.
    # raft_recovery_default_read_size: 524288	#Default size of read issued during raft follower recovery.
    # raft_replicate_batch_window_size: 1048576	#Maximum size of requests cached for replication.
    # raft_smp_max_non_local_requests:   #Maximum number of x-core requests pending in Raft seastar::smp group. (for more details look at seastar::smp_service_group documentation).
    # raft_timeout_now_timeout_ms: 1000   #Timeout for a timeout now request.
    # raft_transfer_leader_recovery_timeout_ms: 1000	#Timeout waiting for follower recovery when transferring leadership.
    # raft_election_timeout_ms: 1500ms                             # Election timeout expressed in milliseconds TBD - election_time_out
    # readers_cache_eviction_timeout_ms: 30s                       # Duration after which inactive readers will be evicted from cache
    # reclaim_growth_window: 3000ms                                # Length of time in which reclaim sizes grow
    # reclaim_max_size: 4MB                                        # Maximum batch cache reclaim size
    # reclaim_min_size: 128KB                                      # Minimum batch cache reclaim size
    # reclaim_stable_window: 10000ms                               # Length of time above which growth is reset
    # recovery_append_timeout_ms: 5s                               # Timeout for append entries requests issued while updating stale follower
    # release_cache_on_segment_roll: false                         # Free cache when segments roll
    # replicate_append_timeout_ms: 3s                              # Timeout for append entries requests issued while replicating entries
    # segment_appender_flush_timeout_ms: 1ms                       # Maximum delay until buffered data is written
    # wait_for_leader_timeout_ms: 5000ms                           # Timeout (ms) to wait for leadership in metadata cache
  node:
    # node_id:                                                     # Unique ID identifying a node in the cluster
    # data_directory:                                              # Place where redpanda will keep the data
    # admin_api_doc_dir: /usr/share/redpanda/admin-api-doc         # Admin API doc directory
    # api_doc_dir: /usr/share/redpanda/proxy-api-doc               # API doc directory
    # coproc_supervisor_server: 127.0.0.1:43189                    # IpAddress and port for supervisor service
    # dashboard_dir: None                                          # serve http dashboard on / url
    # rack: None                                                   # Rack identifier

  # Invalid properties
  # Any of these properties will be ignored. These otherwise valid properties are not allowed
  # to be used in this section since they impact deploying Redpanda in Kubernetes.
  # Make use of the above sections to modify these values instead (see comments below).
  # admin: 127.0.0.1:9644                               # Address and port of admin server
  # admin_api_tls: validate_many                                # TLS configuration for admin HTTP server
  # advertised_kafka_api: None                                # Address of Kafka API published to the clients
  # advertised_pandaproxy_api: None                               # Rest API address and port to publish to client
  # advertised_rpc_api: None                                # Address of RPC endpoint published to other cluster members
  # cloud_storage_access_key: None                                # AWS access key
  # cloud_storage_api_endpoint: None                                # Optional API endpoint
  # cloud_storage_api_endpoint_port: 443                                # TLS port override
  # cloud_storage_bucket: None                                # AWS bucket that should be used to store data
  # cloud_storage_disable_tls: false                               # Disable TLS for all S3 connections
  # cloud_storage_enabled: false                                # Enable archival storage
  # cloud_storage_max_connections: 20                                # Max number of simultaneous uploads to S3
  # cloud_storage_reconciliation_ms: 10s                                # Interval at which the archival service runs reconciliation (ms)
  # cloud_storage_region: None                                # AWS region that houses the bucket used for storage
  # cloud_storage_secret_key: None                                # AWS secret key
  # cloud_storage_trust_file: None                                # Path to certificate that should be used to validate server certificate during TLS handshake
  # default_topic_partitions: 1                                # Default number of partitions per topic
  # default_topic_replications: 3                                # Default replication factor for new topics
  # enable_admin_api                                Enable the admin API                                true
  # enable_sasl                                Enable SASL authentication for Kafka connections                                false
  # kafka_api                                Address and port of an interface to listen for Kafka API requests                                127.0.0.1:9092
  # kafka_api_tls                                TLS configuration for Kafka API endpoint                                None
  # pandaproxy_api                                Rest API listen address and port                                0.0.0.0:8082
  # pandaproxy_api_tls                                TLS configuration for Pandaproxy api                                validate_many
  # rpc_server                                IP address and port for RPC server                                127.0.0.1:33145
  # rpc_server_tls                                TLS configuration for RPC server                                validate
  # seed_servers                                List of the seed servers used to join current cluster; If the seed_server list is empty the node will be a cluster root and it will form a new cluster                                None
  # superusers                                List of superuser usernames                                None

#
# Listener overrides
#
# Use this section to override the per-listener settings configured above
#
listeners:
  # Admin API listener
  # The admin listener group cannot be disabled
  admin:
    # The port for the admin server
    # Metrics are served off of this port at /metrics
    port: 9644
    # Optional external section
    external:
      # Optional keys to override the external access configuration (`external`)
      # for the admin API
      # enabled: true
      # type: NodePort

      # This listener port will be used for the external port if this is not included
      port: 31644
    # Optional TLS section (required if global TLS is enabled)
    tls:
      # Optional flag to override the global TLS enabled flag
      # enabled: true
      # Name of certificate used for TLS (must match a cert registered at auth.tls.certs)
      cert: admin
      # If true, the truststore file for this listener will be included in the ConfigMap
      requireClientAuth: false
  # Kafka API listeners
  # The kafka listener group cannot be disabled
  kafka:
    # The first entry is used to set up the headless service, which enables
    # internal clients within the same namespace to connect directly to brokers.
    # By default there are two listeners, one for internal and another for external clients.
    # A listener with the name `internal` must exist, and will always be advertised
    # using the internal address.
    # Other listeners will have the advertised address according to whether it is
    # externally facing or not.
    endpoints:
    - name: internal
      port: 9093
      external:
        enabled: false
      tls:
        # enabled: true
        cert: kafka
        requireClientAuth: false
    - name: external
      port: 9092
      # Optional external section
      external:
        # enabled: true
        # Type of external access (options are NodePort and LoadBalancer)
        # type: NodePort
        # External port
        # This listener port will be used for the external port if this is not included
        port: 31092
      tls:
        # enabled: true
        cert: kafka
        requireClientAuth: false
  # HTTP API listeners (aka PandaProxy)
  http:
    enabled: true
    # PandaProxy is a kafka client that connects to an endpoint from listeners.kafka.endpoints
    kafkaEndpoint: default
    endpoints:
    - name: default
      port: 8082
    # Optional external section
      external:
        # enabled: true
        # Type of external access (options are NodePort and LoadBalancer)
        # type: NodePort
        # External port
        # This listener port will be used for the external port if this is not included
        port: 30082
      tls:
        # enabled: true
        cert: proxy
        requireClientAuth: false
  # RPC listener
  # The RPC listener cannot be disabled
  rpc:
    port: 33145
    tls:
      # enabled: true
      cert: rpc
      requireClientAuth: false
  # Schema registry listeners
  schemaRegistry:
    enabled: true
    # Schema Registry is a kafka client that connects to an endpoint from listeners.kafka.endpoints
    kafkaEndpoint: default
    endpoints:
    - name: default
      port: 8081
      # Optional external section
      external:
        # enabled: true
        # Type of external access (options are NodePort and LoadBalancer)
        # type: NodePort
        # External port
        # This listener port will be used for the external port if this is not included
        port: 30081
      tls:
        # enabled: true
        cert: schema
        requireClientAuth: false
