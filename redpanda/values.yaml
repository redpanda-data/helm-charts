# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# values.yaml
#
# This file contains values for variables referenced from yaml files in the templates directory.
#
# For further information on Helm templating see the documentation at:
#   https://helm.sh/docs/chart_template_guide/values_files/

image:
  repository: "vectorized/redpanda"
  tag: "v21.11.16"                                                # imagePullPolicy will default to Always when the tag is 'latest'
labels: {}                                                      # additional labels to apply to all Kubernetes resources created by this chart
clusterDomain: "cluster.local"
statefulset:
  replicas: 3
  updateStrategy:
    type: "RollingUpdate"
  podManagementPolicy: "Parallel"
  budget:
    maxUnavailable: 1
  annotations: {}                                               # Additional annotations to apply to the Pods of this StatefulSet.

# Redpanda makes use of a thread per core model which is described here:
# https://redpanda.com/blog/tpc-buffers/ For this reason Redpanda should
# only be given full cores for requests and limits. The recommendation
# for memory for Redpanda is at least 2GB per core. These values will also
# affect the --smp and --memory flags which are passed to Redpanda.
#
# Limits are only specified as to provide Guaranteed QoS:
# https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/#create-a-pod-that-gets-assigned-a-qos-class-of-guaranteed
#
# To improve performance further it is recommended to enable CPU Affinity:
# https://kubernetes.io/docs/tasks/administer-cluster/cpu-management-policies/#static-policy
#
# NOTE: You can increase the number of cores but decreasing the number
# is not supported currently.
# https://github.com/redpanda-data/redpanda/issues/350#
  resources:
    limits:
      cpu: 1
      memory: "2.5Gi"                                           # recommended at least 2Gi per core

  podAffinity: {}                                               # Inter-Pod Affinity rules for scheduling Pods of this StatefulSet.
  podAntiAffinity:                                              # Anti-affinity rules for scheduling Pods of this StatefulSet.
    topologyKey: "kubernetes.io/hostname"                       # The topologyKey to be used.
    type: "soft"                                                # Type of anti-affinity rules: either `soft`, `hard` or empty value (which disables anti-affinity rules).
    weight: 100                                                 # weight for `soft` anti-affinity rules (does not apply for other anti-affinity types)
  nodeSelector: {}                                              # Node selection constraints for scheduling Pods of this StatefulSet.
  priorityClassName: ""                                         # PriorityClassName given to Pods of this StatefulSet
  tolerations: []                                               # Taints to be tolerated by Pods of this StatefulSet.

  topologySpreadConstraints:                                    # https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/
    maxSkew: 1
    topologyKey: "topology.kubernetes.io/zone"
    whenUnsatisfiable: "ScheduleAnyway"

serviceAccount:
  create: false                                                 # Specifies whether a service account should be created
  annotations: {}                                               # Annotations to add to the service account
  name: ""                                                      # service account name; if not set and create is true, a name is generated using the fullname template

podSecurityContext:
  fsGroup: 101                                                  # allow users in redpanda group to access storage

securityContext: {}
#  capabilities:
#    drop:
#    - ALL
#  readOnlyRootFilesystem: true
#  runAsNonRoot: true
#  runAsUser: 1000

storage:
  hostPath: ""                                                  # absolute host path to store Redpanda's data (emptyDir if not specified)
  persistentVolume:
    enabled: true                                               # a PersistentVolumeClaim will be created if true, otherwise hostPath
    size: "100Gi"

# If storageClass is defined, then `storageClassName: <storageClass>`.
# If it is "-", then `storageClassName: ""`, disabling dynamic provisioning.
# If it is undefined or empty (default), then no `storageClassName` spec is set,
# and the default provisioner will be chosen (gp2 on AWS, standard on GKE, AWS & OpenStack).
    storageClass: ""

    labels: {}                                                  # Additional labels to apply to the created PersistentVolumeClaims
    annotations: {}                                             # Additional annotations to apply to the created PersistentVolumeClaims

config:                                                         # see https://docs.redpanda.com/docs/cluster-administration/configuration/
  organization: ""                                              # your organization name
  license_key: ""                                               # license key (if you have one)

  pandaproxy:
    pandaproxy_api:                                             # panda proxy listeners
    - address: "0.0.0.0"
      port: 8082
      name: "pandaproxy"
      external:
        enabled: true

  schema_registry:
    api_doc_dir: "/usr/share/redpanda/proxy-api-doc"          # API doc directory
    schema_registry_replication_factor: ~                     # Replication factor for internal _schemas topic.  If unset, defaults to `default_topic_replication`
    schema_registry_api:
    - address: "0.0.0.0"
      port: 8081
      name: "schema-registry"
      external:
        enabled: true

  seastar:
    default_log_level: "info"

  redpanda:

# Below are the properties for Redpanda separated into node and cluster sections.
# Default values and descriptions are provided for most properties.

# Redpanda node config https://docs.redpanda.com/docs/reference/node-properties/

    # Redpanda memory parameter can not exceed POD resource memory limit
    # Example calculation:
    # statefulset.redpanda.memory = 0.8 statefulset.resources.limits.memory
    #
    # NOTE: Memory flag does not support float numbers and support only E, P, T, G, M, k suffixes
    # REF:
    # https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#meaning-of-memory
    # https://kubernetes.io/docs/reference/kubernetes-api/common-definitions/quantity/
    memory: 2G
    reservedMemory: 400k                                         # Memory flag does not support float numbers; supports E, P, T, G, M, k suffixes
#    node_id: ~                                                  # Unique id identifying a node in the cluster
#    data_directory: "/var/lib/redpanda/data"                    # Place where redpanda will keep the data

    admin:
      address: "0.0.0.0"
      port: 9644
      name: admin
      external:
        enabled: false

    # The first entry is used to set up the headless service.
    # Kafka external and internal advertised listeners will be derived from the configuration.
    # External advertised listener will be via node port unless load balancer is configured.
    kafka_api:
      - address: "0.0.0.0"
        port: 9092
        name: "kafka"
        external:
          enabled: false
#          subdomain: ~

    rpc_server:                                                 # rpc server listener
      address: "0.0.0.0"
      port: 33145
#      external:
#        enabled: false

# Redpanda cluster config https://docs.redpanda.com/docs/reference/cluster-properties

    # admin
#    admin_api_require_auth: false                               # Whether admin API clients must provide HTTP Basic authentication headers
#    superusers: []                                              # List of superuser usernames
#
    # shadow indexing
#    cloud_storage_enabled: false                                # Enable archival storage
#    cloud_storage_region: ~                                     # S3 region that houses the bucket used for storage
#    cloud_storage_bucket: ~                                     # S3 bucket that should be used to store data
#    cloud_storage_api_endpoint: ~                               # S3 API endpoint
#    cloud_storage_api_endpoint_port: 443                        # S3 API port
#    cloud_storage_access_key: ~                                 # S3 access key
#    cloud_storage_secret_key: ~                                 # S3 secret key
#    cloud_storage_enable_remote_write: false                    # Enable remote write for all topics
#    cloud_storage_enable_remote_read: false                     # Enable remote read for all topics
#    cloud_storage_segment_max_upload_interval_sec: ~            # seconds between segment uploads (else segments uploaded when size == segment.bytes)
#    cloud_storage_disable_tls: false                            # disable TLS (set to true to disable TLS, or if TLS is handled by the proxy)
#    cloud_storage_initial_backoff_ms: 100                       # Initial backoff time for exponetial backoff algorithm (ms)
#    cloud_storage_cache_check_interval: ~                       # Timeout to check if cache eviction should be triggered
#    cloud_storage_cache_size: 21474836480                       # Max size of archival cache
#    cloud_storage_manifest_upload_timeout_ms: 10000             # Manifest upload timeout (ms)
#    cloud_storage_max_connections: 20                           # Max number of simultaneous uploads to S3
#    cloud_storage_max_connection_idle_time_ms: 5000             # Max https connection idle time (ms)
#    cloud_storage_reconciliation_interval_ms: 1000              # Interval at which the archival service runs reconciliation (ms)
#    cloud_storage_segment_upload_timeout_ms: 30000              # Log segment upload timeout (ms)
#    cloud_storage_trust_file: ~                                 # Path to certificate that should be used to validate server certificate during TLS handshake
#    cloud_storage_upload_ctrl_d_coeff: 0                        # derivative coefficient for upload PID controller.
#    cloud_storage_upload_ctrl_max_shares: 1000                  # maximum number of IO and CPU shares that archival upload can use
#    cloud_storage_upload_ctrl_min_shares: 100                   # minimum number of IO and CPU shares that archival upload can use
#    cloud_storage_upload_ctrl_p_coeff: -2                       # proportional coefficient for upload PID controller
#    cloud_storage_upload_ctrl_update_interval_ms: 60000
#    cloud_storage_upload_loop_initial_backoff_ms: 100           # Initial backoff interval when there is nothing to upload for a partition (ms)
#    cloud_storage_upload_loop_max_backoff_ms: 10000             # Max backoff interval when there is nothing to upload for a partition (ms)
#
    # cluster management
#    cluster_id: ~                                               # Cluster identifier
#    enable_auto_rebalance_on_node_add: false                    # Enable automatic partition rebalancing when new nodes are added
#    enable_leader_balancer: true                                # Enable automatic leadership rebalancing
#    enable_rack_awareness: false                                # Enables rack-aware replica assignment
#
    # Kafka API
#    enable_idempotence: true                                    # Enable idempotent producer
#    enable_sasl: false                                          # Enable SASL authentication for Kafka connections.
#    fetch_max_bytes: 57671680                                   # Maximum number of bytes returned in fetch request
#    group_min_session_timeout_ms: 6000                          # The minimum allowed session timeout for registered consumers. Shorter timeouts result in quicker failure detection at the cost of more frequent consumer heartbeating, which can overwhelm broker resources.
#    kafka_connection_rate_limit: ~                              # Maximum connections per second for one core (minimum 1)
#    kafka_connection_rate_limit_overrides: ~                    # Overrides for specific ips for maximum connections per second for one core
#    kafka_group_recovery_timeout_ms: 30000                      # Kafka group recovery timeout expressed in milliseconds
#    kafka_qdc_enable: false                                     # Enable kafka queue depth control.
#    kafka_qdc_max_latency_ms: 80                                # Max latency threshold for kafka queue depth control depth tracking.
#    rm_sync_timeout_ms: 10000                                   # Time to wait state catch up before rejecting a request
#    rpc_server_listen_backlog: ~                                # TCP connection queue length for Kafka server and internal RPC server (minimum 1)
#    rpc_server_tcp_recv_buf: ~                                  # TCP receive buffer size in bytes (minimum 32768)
#    rpc_server_tcp_send_buf: ~                                  # TCP transmit buffer size in bytes (minimum 32768)
#    target_quota_byte_rate: 2147483648                          # Target quota byte rate in bytes per second (minimum 1048576)
#    kafka_connections_max: ~                                    # Maximum number of Kafka client connections per broker
#    kafka_connections_max_overrides: ~                          # Per-IP overrides of kafka connection count limit, list of <ip>:<count> strings
#    kafka_connections_max_per_ip: ~                             # Maximum number of Kafka client connections from each IP address, per broker
#    kafka_max_bytes_per_fetch: 67108864                         # Limit fetch responses to this many bytes, even if total of partition bytes limits is higher
#    kafka_qdc_depth_alpha: 0.8                                  # Smoothing factor for kafka queue depth control depth tracking.
#    kafka_qdc_depth_update_ms: 7000                             # Update frequency for kafka queue depth control.
#    kafka_qdc_idle_depth: 10                                    # Queue depth when idleness is detected in kafka queue depth control.
#    kafka_qdc_latency_alpha: 0.002                              # Smoothing parameter for kafka queue depth control latency tracking.
#    kafka_qdc_max_depth: 100                                    # Maximum queue depth used in kafka queue depth control.
#    kafka_qdc_min_depth: 1                                      # Minimum queue depth used in kafka queue depth control.
#    kafka_qdc_window_count: 12                                  # Number of windows used in kafka queue depth control latency tracking.
#    kafka_qdc_window_size_ms: 1500                              # Window size for kafka queue depth control latency tracking.
#    max_kafka_throttle_delay_ms: 60000                          # Fail-safe maximum throttle delay on kafka requests
#
    # metrics
#    disable_metrics: false                                      # Disable registering metric
#    enable_metrics_reporter: true                               # Enable cluster metrics reporter
#    metrics_reporter_report_interval: 86400000                  # cluster metrics reporter report interval
#    metrics_reporter_tick_interval: 60000                       # Cluster metrics reporter tick interval
#    metrics_reporter_url: "https://m.rp.vectorized.io/v2"       # cluster metrics reporter url
#
    # raft
#    raft_learner_recovery_rate: 104857600                       # Raft learner recovery rate limit in bytes per sec
#    full_raft_configuration_recovery_pattern: ~                 # Recover raft configuration on start for NTPs matching pattern
#    raft_heartbeat_disconnect_failures: 3                       # After how many failed heartbeats to forcibly close an unresponsive TCP connection.  Set to 0 to disable force disconnection.
#    raft_heartbeat_interval_ms: 150                             # Milliseconds for raft leader heartbeats
#    raft_heartbeat_timeout_ms: 3000                             # raft heartbeat RPC timeout
#    raft_io_timeout_ms: 10000                                   # Raft I/O timeout
#    raft_max_concurrent_append_requests_per_follower: 16        # Maximum number of concurrent append entries requests sent by leader to one follower
#    raft_max_recovery_memory: ~                                 # Max memory that can be used for reads in raft recovery process by default 15% of total memory
#    raft_recovery_default_read_size: 524288                     # default size of read issued during raft follower recovery
#    raft_replicate_batch_window_size: 1048576                   # Max size of requests cached for replication
#    raft_smp_max_non_local_requests: 0                          # Maximum number of x-core requests pending in Raft seastar::smp group. (for more details look at `seastar::smp_service_group` documentation)
#    raft_timeout_now_timeout_ms: 1000                           # Timeout for a timeout now request
#    raft_transfer_leader_recovery_timeout_ms: 10000             # Timeout waiting for follower recovery when transferring leadership
#
    # writing to disk
#    delete_retention_ms: 604800000                              # delete segments older than this, default 1 week
#    log_cleanup_policy: "delete"                                # Default topic cleanup policy
#    log_compaction_interval_ms: 10000                           # How often do we trigger background compaction
#    log_compression_type: "producer"                            # Default topic compression type
#    log_message_timestamp_type: "CreateTime"                    # Default topic messages timestamp type
#    log_segment_size: 1073741824                                # How large in bytes should each log segment be (default 1G)
#    max_compacted_log_segment_size: 5368709120                  # Max compacted segment size after consolidation
#    compacted_log_segment_size: 268435456                       # How large in bytes should each compacted log segment be (default 256MiB)
#
    # topics and partitions
#    auto_create_topics_enabled: false                           # Allow topic auto creatio
#    default_topic_partitions: 1                                 # Default number of partitions per topic
#    default_topic_replications: 1                               # Default replication factor for new topics
#    id_allocator_replication: 1                                 # Replication factor for an id allocator topic
#    internal_topic_replication_factor: 3                        # Target replication factor for internal topics
#    retention_bytes: ~                                          # Default max bytes per partition on disk before triggering a compaction
#    rm_violation_recovery_policy: "crash"                       # Describes how to recover from an invariant violation that happened on the partition level
#    create_topic_timeout_ms: 2000                               # Timeout (ms) to wait for new topic creation
#    topic_memory_per_partition: 1048576                         # Required memory per partition when creating topics
#
    # transactions
#    enable_transactions: false                                  # Enable transactions
#    seq_table_min_size: 1000                                    # Minimum size of the seq table non affected by compaction
#    tm_sync_timeout_ms: 10000                                   # Time to wait state catch up before rejecting a request
#    tm_violation_recovery_policy: "crash"                       # Describes how to recover from an invariant violation happened on the transaction coordinator level
#    transaction_coordinator_cleanup_policy: "delete"            # Cleanup policy for a transaction coordinator topic
#    transaction_coordinator_delete_retention_ms: 604800000      # delete segments older than this - default 1 week
#    transaction_coordinator_replication: 1                      # Replication factor for a transaction coordinator topic
#    transactional_id_expiration_ms: 604800000                   # Producer ids are expired once this time has elapsed after the last write with the given producer id.
#    tx_timeout_delay_ms: 1000                                   # Delay before scheduling next check for timed out transactions
#    abort_timed_out_transactions_interval_ms: 60000             # How often look for the inactive transactions and abort them
#    transaction_coordinator_log_segment_size: 1073741824        # How large in bytes should each log segment be (default 1G)
#
    # wasm
#    enable_coproc: false                                        # Enable coprocessing mode
#    coproc_max_batch_size: 32768                                # Maximum amount of bytes to read from one topic read
#    coproc_max_ingest_bytes: 655360                             # Maximum amount of data to hold from input logs in memory
#    coproc_max_inflight_bytes: 10485760                         # Maximum amount of inflight bytes when sending data to wasm engine
#    coproc_offset_flush_interval_ms: 300000                     # Interval for which all coprocessor offsets are flushed to disk
#
    # other
#    abort_index_segment_size: 50000                             # Capacity (in number of txns) of an abort index segment
#    alter_topic_cfg_timeout_ms: 5000                            # Time to wait for entries replication in controller log when executing alter configuration requst
#    append_chunk_size: 16384                                    # Size of direct write operations to disk in bytes
#    compaction_ctrl_backlog_size: ~                             # target backlog size for compaction controller. if not set compaction target compaction backlog would be equal to
#    compaction_ctrl_d_coeff: 0.2                                # derivative coefficient for compaction PID controller.
#    compaction_ctrl_i_coeff: 0                                  # integral coefficient for compaction PID controller.
#    compaction_ctrl_max_shares: 1000                            # maximum number of IO and CPU shares that compaction process can use
#    compaction_ctrl_min_shares: 10                              # minimum number of IO and CPU shares that compaction process can use
#    compaction_ctrl_p_coeff: -12.5                              # proportional coefficient for compaction PID controller. This has to be negative since compaction backlog should decrease when number of compaction shares increases
#    compaction_ctrl_update_interval_ms: 30000
#    controller_backend_housekeeping_interval_ms: 1000           # Interval between iterations of controller backend housekeeping loop
#    default_num_windows: 10                                     # Default number of quota tracking windows
#    default_window_sec: 1000                                    # Default quota tracking window size in milliseconds
#    disable_batch_cache: false                                  # Disable batch cache in log manager
#    election_timeout_ms: 1500                                   # Election timeout expressed in milliseconds
#    enable_pid_file: true                                       # Enable pid file. You probably don't want to change this.
#    features_auto_enable: true                                  # Whether new feature flags may auto-activate after upgrades (true) or must wait for manual activation via the admin API (false)
#    fetch_reads_debounce_timeout: 1                             # Time to wait for next read in fetch request when requested min bytes wasn't reached
#    fetch_session_eviction_timeout_ms: 60000                    # Minimum time before which unused session will get evicted from sessions. Maximum time after which inactive session will be deleted is two time given configuration valuecache
#    group_initial_rebalance_delay: 300                          # Extra delay (ms) added to rebalance phase to wait for new members
#    group_max_session_timeout_ms: 300000                        # The maximum allowed session timeout for registered consumers. Longer timeouts give consumers more time to process messages in between heartbeats at the cost of a longer time to detect failures.
#    group_new_member_join_timeout: 30000                        # Timeout for new member joins
#    group_topic_partitions: 1                                   # Number of partitions in the internal group membership topic
#    health_manager_tick_interval: 180000                        # How often the health manager runs
#    health_monitor_max_metadata_age: 10000                      # Max age of metadata cached in the health monitor of non controller node
#    health_monitor_tick_interval: 10000                         # How often health monitor refresh cluster state
#    id_allocator_batch_size: 1000                               # Id allocator allocates messages in batches (each batch is a one log record) and then serves requests from memory without touching the log until the batch is exhausted.
#    id_allocator_log_capacity: 100                              # Capacity of the id_allocator log in number of messages. Once it reached id_allocator_stm should compact the log.
#    join_retry_timeout_ms: 5000                                 # Time between cluster join retries in milliseconds
#    kvstore_flush_interval: 10                                  # Key-value store flush interval (ms)
#    kvstore_max_segment_size: 16777216                          # Key-value maximum segment size (bytes)
#    leader_balancer_idle_timeout: 120000                        # Leadership rebalancing idle timeout
#    leader_balancer_mute_timeout: 300000                        # Leadership rebalancing mute timeout
#    max_version: ~
#    members_backend_retry_ms: 5000                              # Time between members backend reconciliation loop retries
#    metadata_dissemination_interval_ms: 3000                    # Interaval for metadata dissemination batching
#    metadata_dissemination_retries: 30                          # Number of attempts of looking up a topic's meta data like shard before failing a request
#    metadata_dissemination_retry_delay_ms: 320                  # Delay before retry a topic lookup in a shard or other meta tables
#    metadata_status_wait_timeout_ms: 2000                       # Maximum time to wait in metadata request for cluster health to be refreshed
#    min_version: ~
#    node_management_operation_timeout_ms: 5000                  # Timeout for executing node management operations
#    quota_manager_gc_sec: 30000                                 # Quota manager GC frequency in milliseconds
#    readers_cache_eviction_timeout_ms: 30000                    # Duration after which inactive readers will be evicted from cache
#    reclaim_batch_cache_min_free: 67108864                      # Free memory limit that will be kept by batch cache background reclaimer
#    reclaim_growth_window: 3000                                 # Length of time in which reclaim sizes grow
#    reclaim_max_size: 4194304                                   # Maximum batch cache reclaim size
#    reclaim_min_size: 131072                                    # Minimum batch cache reclaim size
#    reclaim_stable_window: 10000                                # Length of time above which growth is reset
#    recovery_append_timeout_ms: 5000                            # Timeout for append entries requests issued while updating stale follower
#    release_cache_on_segment_roll: false                        # Free cache when segments roll
#    replicate_append_timeout_ms: 3000                           # Timeout for append entries requests issued while replicating entries
#    seed_server_meta_topic_partitions:
#    segment_appender_flush_timeout_ms: 1000                     # Maximum delay until buffered data is written
#    segment_fallocation_step: 33554432                          # Size for segments fallocation
#    storage_read_buffer_size: 131072                            # Size of each read buffer (one per in-flight read, per log segment)
#    storage_read_readahead_count: 10                            # How many additional reads to issue ahead of current read location
#    storage_space_alert_free_threshold_bytes: 1073741824        # Threshold of minimim bytes free space before setting storage space alert
#    storage_space_alert_free_threshold_percent: 5               # Threshold of minimim percent free space before setting storage space alert
#    topic_fds_per_partition: 10                                 # Required file handles per partition when creating topics
#    use_scheduling_groups:
#    wait_for_leader_timeout_ms: 5000                            # Timeout (ms) to wait for leadership in metadata cache
#    zstd_decompress_workspace_bytes: 8388608                    # Size of the zstd decompression workspace
